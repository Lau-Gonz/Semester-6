{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <h1 style=\"color:blue;text-align:left\">Inteligencia Artificial</h1></td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Notebook</p></tp>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Arquitecturas de agente</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook ejemplificaremos dos tipos de agente: dirigido por tabla y de respuesta simple. Usaremos el problema del laberinto para implementar los programa de agente y mostraremos una manera de comparar sus desempeños.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al ejercicio 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias\n",
    "\n",
    "Al iniciar el notebook o reiniciar el kerner se pueden cargar todas las dependencias de este notebook corriendo las siguientes celdas. Este también es el lugar para instalar las dependencias que podrían hacer falta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Del notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentes import Random, TableDriven, RuleBased\n",
    "from ambientes import Laberinto\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secciones\n",
    "\n",
    "Desarrollaremos la explicación en las siguientes secciones:\n",
    "\n",
    "1. [El ambiente del laberinto](#lab)\n",
    "2. [Un agente dirigido por tabla](#agenteTD)\n",
    "3. [Un agente de reflejo simple](#agenteSR)\n",
    "4. [Pruebas de desempeño](#pruebas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ambiente del laberinto <a class=\"anchor\" id=\"lab\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Imagine un agente dentro de un laberinto, el cual consiste de una rejilla de doce por doce casillas. El laberinto tiene solo una casilla que sirve de entrada y de salida, que es la casilla $(0,0)$. Observe que en la notación de las casillas, la primera coordenada representa la posición en el ejer horizontal y la segunda en el vertical, con origen la esquina inferior izquierda. Esta casilla es la única forma de salir o entrar al laberinto. Cada rejilla es o bien un pasadizo o bien un muro. El agente puede moverse una casilla en dirección vertical u horizontal (nunca diagonal) siempre y cuando la casilla a la que se mueve no sea un muro o el fin de la rejilla. El problema que debe resolver el agente es encontrar un camino que lo lleve hasta la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Laberinto()\n",
    "lab.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La percepción del agente consiste en una colección de sensores que se encienden cuando detectan un obstáculo, organizados de la siguiente manera:\n",
    "\n",
    "`[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]`\n",
    "\n",
    "Los estados del `Laberinto` los constituyen los valores de los sensores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = Laberinto()\n",
    "state = lab.reset()\n",
    "print('El estado inicial es:')\n",
    "print(f'[sensor frontal={state[0]}, sensor izquierdo={state[1]}, sensor derecho={state[2]}, sensor trasero={state[3]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las acciones posibles del agente son:\n",
    "\n",
    "* adelante: el agente avanza una casilla.\n",
    "* voltearIzquierda: el agente gira 90º en contra de las manecillas del reloj.\n",
    "* voltearDerecha: el agente gira 90º a favor de las manecillas del reloj.\n",
    "\n",
    "Cada acción del agente tiene un efecto en el entorno, implementado mediante el método `step(action)` de la clase `Laberinto`. Este método recibe la acción del agente y devuelve un nuevo estado, una recompensa, y la información de si el entorno sigue activo. Este es el ciclo básico agente-entorno, ejemplificado en el siguiente diagrama:\n",
    "\n",
    "<img src=\"./imagenes/agent_loop.png\" width=\"400\"/>\n",
    "\n",
    "En el módulo `utils` tenemos una serie de clases que nos harán la vida más fácil a la hora de correr los agentes en el entorno. La colección de rondas desde el estado inicial hasta el estado final o hasta el número máximo de rondas se llama un **episodio**. La clase que usaremos para correr un episodio se llama `Episode`. Veamos un ejemplo en donde un agente aleatorio, implementado en la clase `Random` del módulo `agentes`, parte de la casilla $(11,11)$ y deambula aleatoriamente por el laberinto por 15 rondas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "lab = Laberinto()\n",
    "# Create agent\n",
    "agent = Random()\n",
    "# Create episode\n",
    "episodio = utils.Episode(environment=lab,\\\n",
    "        agent=agent,\\\n",
    "        model_name='Random',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente dirigido por tabla <a class=\"anchor\" id=\"agenteTD\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El tipo más sencillo de un programa de agente es cuando hacemos una asociación directa entre input y output, en este caso, entre percepción y acción. Esta asociación se puede realizar mediante una tabla. \n",
    "\n",
    "Definimos la siguiente tabla (que hasta ahora sólo está definida parcialmente), la cual vincula perceptos con acciones. La tabla implementa la idea de que si el agente percibe que el frente no está bloqueado y el flanco derecho está bloqueado, entonces avanza una casilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sensor frontal, sensor izquierdo, sensor derecho, sensor trasero)\n",
    "tabla = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    (False, True, True, True):['adelante'],\n",
    "    (False, True, True, False):['adelante'],\n",
    "    (False, False, True, True):['adelante'],\n",
    "    (False, False, True, False):['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que esta regla tan simple requiere ser expresada mediante cuatro filas de la tabla.\n",
    "\n",
    "Observemos cómo trabaja el agente que implementa este programa, que se encuentra implementado en la clase `TableDriven` del módulo `agentes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "lab = Laberinto()\n",
    "# Create agent\n",
    "agent = TableDriven()\n",
    "agent.tabla = tabla\n",
    "# Create episode\n",
    "episodio = utils.Episode(environment=lab,\\\n",
    "        agent=agent,\\\n",
    "        model_name='Tabla',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `¡Tabla incompleta! No contempla el estado [True, False, True, False]` ocurre porque la tabla no tiene ninguna fila para el percepto `(True, False, True, False)` y entonces no puede determinar ninguna acción a tomar. ¡Observe que el agente no sabe qué acción tomar en ninguna situación cuando hay un muro enfrente! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "El agente llega hasta que se topa con un muro y no sabe qué hacer. Extienda la tabla anterior para incluir las líneas que determinan que \"si el frente y el flanco derecho están bloqueados y el flanco izquierdo no está bloqueado, voltear a la izquierda\".\n",
    "\n",
    "Visualice el funcionamiento del agente para comprobar su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "En la tabla falta incluir instrucciones que digan que \"si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla\". Extienda la tabla del ejercicio 2 para incluir las líneas que implementan esta regla. Visualice el funcionamiento del agente comenzando desde la casilla $(11,11)$.\n",
    "\n",
    "**Nota:** cambie el parámetro `num_rounds` del episodio a 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(6,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso.\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(7,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente de reflejo simple <a class=\"anchor\" id=\"agenteSR\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El proceso de escribir una tabla es bastante dispendioso, pues hay que considerar una gran cantidad de combinaciones de perceptos de los sensores. Un tipo de agente de nivel un poco más elevado que resuelve esta situación son los agentes de reflejo simple. El programa de este tipo de agentes está basado en relgas de condición-acción, las cuales relacionan condiciones sobre los sensores y las acciones. Al considerar condiciones en lugar de combinaciones de perceptos, la escritura es más eficiente.\n",
    "\n",
    "A continuación presentamos una posible implementación de un agente de reflejo simple para el problema del laberinto, implementado en la clase `RuleBased` del módulo `agentes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state[0]  =>  sensor forntal\n",
    "# state[1]  =>  sensor izquierdo\n",
    "# state[2]  =>  sensor derecho\n",
    "# state[3]  =>  sensor trasero\n",
    "reglas = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    'not state[0] and state[2]': ['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "lab = Laberinto()\n",
    "# Create agent\n",
    "agent = RuleBased()\n",
    "agent.reglas = reglas\n",
    "# Create episode\n",
    "episodio = utils.Episode(environment=lab,\\\n",
    "        agent=agent,\\\n",
    "        model_name='Reglas',\\\n",
    "        num_rounds=70)\n",
    "# Visualize\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `¡Plan vacío! Revisar reglas en estado [True, False, True, False]` ocurre porque el programa aún no está equipado para dar una decisión cuando haya un muro enfrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3)) ([Próximo ejercicio](#ej5))\n",
    "\n",
    "Extienda el programa de agente anterior para implementar las reglas condición-acción siguientes:\n",
    "\n",
    "* Si el frente y el flanco derecho están bloqueados pero el flanco izquierdo no está bloqueado, voltear a la izquierda.\n",
    "* Si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla.\n",
    "* Si solo la casilla trasera no está bloqueada, voltear dos veces a la izquierda y avanzar una casilla.\n",
    "\n",
    "Visualice el funcionamiento del agente desde la casilla $(11,11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de desempeño <a class=\"anchor\" id=\"pruebas\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a evaluar el desempeño de un solo agente respecto a qué tan rápido encuentra la salida y qué proporción de veces encuentra la salida. Luego compararemos el desempeño de dos agentes respecto a estas medidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación de un agente**\n",
    "\n",
    "En la clase `Experiment` del módulo `utils` tenemos el método `run_experiment()`, el cual nos sirve para simular varios episodios. En cada episodio el agente inicia en un lugar distinto del laberinto. Esto se logra cambiando a `True` el parámetro de inicialización de la clase `Laberinto`.\n",
    "\n",
    "Corra varias veces la siguiente celda para comprobar que el agente comienza en distintas casillas cada vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "lab = Laberinto(aleatorio=True)\n",
    "lab.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora correremos la simulación. Nuestros parámetros serán un máximo de 300 rondas por cada episodio y correremos 100 episodios.\n",
    "\n",
    "Observe que vamos a obtener un error del agente que tendremos que atender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp = utils.Experiment(num_rounds=300, \\\n",
    "                       num_episodes=100)\n",
    "# Create and load environment\n",
    "lab = Laberinto(aleatorio=True)\n",
    "exp.load_env(lab, \"laberinto\")\n",
    "# Create list of agents\n",
    "agent_rules = RuleBased()\n",
    "agent_rules.reglas = reglas\n",
    "agents_list = [agent_rules]\n",
    "names = ['Reglas']\n",
    "# Run the experiment\n",
    "exp.run_experiment(agents=agents_list, \\\n",
    "                   names=names, \\\n",
    "                   measures=['histogram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma nos muestra una columna muy alta para valores de -300 en la variable de recompensa total (sum of rewards), y otras columnas más pequeñas para valores más cercanos a 0. Observe que la recompensa total es igual al número de pasos que da el agente en el laberinto. Los resultados numéricos que arroja la celda anterior incluyen el promedio de este número de pasos. Observe también que el porcentaje de episodios en los cuales el entorno finaliza antes de las 300 rondas se presenta como el porcentaje de éxito. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparación de varios agentes**\n",
    "\n",
    "Usaremos el método `run_experiment()` con una lista de varios agentes. El entorno se corre para cada uno de los agentes y se guardan los desempeños, para finalmente presentar los resultados de manera gráfica y tabular.\n",
    "\n",
    "En el siguiente ejemplo compararemos el agente aleatorio y el basado en reglas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp = utils.Experiment(num_rounds=300, \\\n",
    "                       num_episodes=100)\n",
    "# Create and load environment\n",
    "# exp.load_test_suite(\"./testsuite_laberinto.json\")\n",
    "lab = Laberinto(aleatorio=True)\n",
    "exp.load_env(lab, \"laberinto\")\n",
    "# Create list of agents\n",
    "agent_random = Random()\n",
    "agent_rules = RuleBased()\n",
    "agent_rules.reglas = reglas\n",
    "agents_list = [agent_random, agent_rules]\n",
    "names = ['Aleatorio', 'Reglas']\n",
    "# Run the experiment\n",
    "exp.run_experiment(agents=agents_list, \\\n",
    "                   names=names, \\\n",
    "                   measures=['histogram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente basado en reglas tiene un mejor desempeño que el agente aleatorio. Observe que la recompensa promedio del agente basado en reglas es -188, superior a -271 del agente aleatorio. Adicionalmente, el porcentaje de éxito del primero es 47%, mucho mayor que 14% del aleatorio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej5\"></a>**Ejercicio 5:** \n",
    "\n",
    "([Anterior ejercicio](#ej4)) ([Próximo ejercicio](#ej6))\n",
    "\n",
    "Compare el desempeño de los agentes dirigido por tabla y basado en reglas. Comente los resultados del histograma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej6\"></a>**Ejercicio 6:** \n",
    "\n",
    "([Anterior ejercicio](#ej5))\n",
    "\n",
    "1. Modifique el agente basado en reglas para implementar las siguientes reglas:\n",
    "    * Si el frente no está bloqueado, avanzar una casilla.\n",
    "    * Si el frente está bloqueado y el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla.\n",
    "    * Si el frente y el flanco derecho están bloqueados y el flanco izquierdo no está bloqueado, voltear a la izquierda.\n",
    "    * Si solo la espalda no está bloqueada, voltear a la izquierda dos veces y avanzar una casilla.\n",
    "\n",
    "2. Compare y comente el desempeño de los dos agentes basados en reglas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook usted aprendió\n",
    "\n",
    "* Los detalles de la implementación de programas de agente dirigido por tabla y de reflejo simple.\n",
    "* Realizar pruebas de desempeño a los agentes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
