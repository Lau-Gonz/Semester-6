{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <h1 style=\"color:blue;text-align:left\">Inteligencia Artificial</h1></td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Juegos</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "## Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook veremos el problema de ganar un juego competitivo contra un humano como el desafío que debe superar una máquina y así, al hacerlo, exhibir una dosis de inteligencia. El desafío es que el entorno es dinámico, toda vez que éste cambiará de acuerdo a las acciones del contrincante. \n",
    "\n",
    "Estudiaremos algunas estrategias para la búsqueda de la mejor jugada en juegos de dos jugadores. Estos escenarios, a pesar de ser dinámicos, todavía son muy convenientes, toda vez que corresponden a entornos completamente observables, discretos y conocidos. \n",
    "\n",
    "También veremos cómo hacer más eficiente el tiempo de búsqueda, al podar el árbol de confrontación. Haremos una poda a lo ancho mediante el algoritmo alfa-beta y una poda a lo alto mediante un cutoff y el uso de funciones de evaluación, las cuales usan información de expertos para evaluar los tableros.\n",
    "\n",
    "En este notebook también estudiaremos cómo evaluar un tablero  cuando no contamos con suficiente conocimiento experto y no sabemos cómo definir una función de evaluación adecuada. La estrategia que estudiaremos se llama la búsqueda en árboles Monte Carlo (Monte Carlo Tree Search). \n",
    "\n",
    "Adaptado de (Russell & Norvig, 2020), secciones 5.1, 5.2, 5.3 y 5.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir a ejercicio 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias\n",
    "\n",
    "Al iniciar el notebook o reiniciar el kerner se pueden cargar todas las dependencias de este notebook corriendo las siguientes celdas. Este también es el lugar para instalar las dependencias que podrían hacer falta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Del notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from juegos import Triqui, ReyTorreRey\n",
    "from tiempos import compara_funciones\n",
    "from utils import *\n",
    "from agents import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "## Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "1. [Definición formal del problema de confrontación](#triqui).\n",
    "2. [Algoritmo minimax](#minimax).\n",
    "    - [Árbol de confrontación](#arbol)\n",
    "    - [Pseudocódigo](#codigo)\n",
    "    - [Engine](#engine)\n",
    "3. [Algoritmo de poda alfa-beta](#alfa-beta).\n",
    "    - [Pseudocódigo](#codigo-a-b)\n",
    "    - [Engine](#engine-a-b)\n",
    "4. [Funciones de evaluación](#feval)\n",
    "5. [Monte Carlo Tree Search](#mcts)\n",
    "    - [Simulación de juego](#sim)\n",
    "    - [Selección de nodos](#seleccion).\n",
    "    - [Expansión del árbol](#expansion).\n",
    "    - [Back-up del valor](#back-up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición formal del problema de confrontación en juegos competitivos <a class=\"anchor\" id=\"triqui\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Mostraremos a continuación la **definición formal de un juego competitivo**. Para ello, nos valdremos de un ejemplo. Consideraremos el juego de tres en línea (o triqui), que todos conocemos desde niños. La definición formal se hace con base en las siguientes características:\n",
    "\n",
    "* **estado_inicial:** Situación del entorno desde el cual comienza el juego. En el caso del triqui, el estado inicial es el tablero $3\\times 3$ vacío.\n",
    "\n",
    "* **player(s):** Define cuál jugador tiene el turno en el estado `s`, el cual puede ser `O` o `X`.\n",
    "\n",
    "* **acciones(s):** Descripción de las posibles acciones del jugador dado por `a_jugar(s)` en el estado `s`. En este caso, poner o bien una `O` o bien una `X` en una casilla vacía.\n",
    "\n",
    "* **resultado(s, a):** Descripción del entorno que resulta de la ejecución de la acción `a` por el jugador dado por `a_jugar(s)` en el estado `s`. \n",
    "\n",
    "* **es_terminal(s):** Permite determinar si el juego se termina cuando se obtiene el estado `s`. \n",
    "\n",
    "* **utilidad(s, j):** Función definida sólo para aquellos estados `s` en los cuales el juego se termina. Esta función establece la utilidad en el estado `s` que se obtiene después de haber jugado el jugador `j`. En nuestro caso del triqui, asumiremos que si el ganador es $O$, la utilidad es -1; si el ganador es $X$, la utilidad es 1; y en caso de empate la utilidad es 0.\n",
    "\n",
    "Asumiremos la siguiente manera de referirnos a las casillas del tablero:\n",
    "\n",
    "<img src=\"imagenes/triqui.png\" width=\"300px\">\n",
    "\n",
    "La siguiente es una posible implementación en Python del ambiente de tarea para el triqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "a = choice(tri.acciones(s))\n",
    "print(\"Este es el tablero con una X en\", a)\n",
    "s1 = tri.resultado(s, a)\n",
    "tri.render(s1)\n",
    "print(\"Juego terminado?:\", tri.es_terminal(s1)) # Debe ser False\n",
    "print(\"Le corresponde el turno a:\", 'O' if tri.player(s1) == 1 else 'X') # Debe ser O\n",
    "print(\"Utilidad:\", tri.utilidad(s1, tri.player(s1))) # Debe ser None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo minimax <a class=\"anchor\" id=\"minimax\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Debemos ahora definir un programa para el agente que juega triqui. Este programa debe consistir, de manera general, en una función que percibe un tablero de triqui y devuelve una acción. Más concretamente, el agente recibe un estado `s` y devuelve una jugada, en forma de un índice `(x,y)` que representa la casilla en la cual ha de ponerse el símbolo correspondiente (o bien $O$ o bien $X$). \n",
    "\n",
    "El algoritmo que utilizaremos para encontrar la mejor jugada se llama *minimax*. Este nombre es muy acertado, toda vez que evoca simultáneamente la confrontación entre los jugadores así como la manera como se representa dicha confrontación. Esto es, un jugador buscará el mínimo y el otro el máximo de la utilidad. Esta es la razón por la cual se definió la función de utilidad de tal manera que un estado en el que gane $O$ valga -1 y uno en que gane $X$ valga 1. El primer jugador será MIN y el segundo MAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de confrontación <a class=\"anchor\" id=\"arbol\"></a>\n",
    "\n",
    "([Volver a minimax](#minimax))\n",
    "\n",
    "Dado un estado `s`, se define un árbol de confrontación para `s` de la siguiente manera. Se consideran para el jugador que tiene el turno, digamos MAX, todas las acciones `i` que se pueden aplicar, y para cada una de ellas se encuentra el correspondiente estado `s`$_i$. Luego, para cada uno de estos, se consideran para MIN todas las acciones que se pueden aplicar y se encuentran los correspondientes estados. Esta iteración se repite hasta llegar a los estados en los cuales el juego se termina. En estos estados podemos hallar la utilidad y aquí comenzamos a encontrar los valores minimax de todo el arbol. Es decir, hemos construido el árbol de arriba hacia abajo (de la raíz hacia las hojas) siguiendo las acciones posibles. Ahora encontraremos los valores minimax para cada estado yendo de abajo hacia arriba (de las hojas hacia la raíz). Así pues, obtenemos los valores minimax a partir de la utilidad de los estados terminales, por ejemplo:\n",
    "\n",
    "<img src=\"imagenes/ejemplo1.png\" width=\"250px\">\n",
    "\n",
    "Suponiendo que estos estados provienen de una acción de las $X$, ellos provienen de estados más arriba en el árbol, como por ejemplo:\n",
    "\n",
    "<img src=\"imagenes/ejemplo2.png\" width=\"250px\">\n",
    "\n",
    "Como la acción en dichos estados es de $X$, que es MAX, entonces el valor minimax se obtiene al encontrar el máximo dentro de todas las opciones posibles. Esto no es interesante en este caso, pues sólo hay una acción posible para $X$, el cual determina el valor minimax para dicho estado.\n",
    "\n",
    "El siguiente nivel hacia arriba es más interesante, pues $O$ (es decir, MIN), tiene varias opciones:\n",
    "\n",
    "<img src=\"imagenes/ejemplo3.png\" width=\"260px\">\n",
    "\n",
    "En este caso $O$ tiene dos opciones, cada una reportando un valor minimax. El valor que MIN escogerá es el mínimo entre los dos valores, es decir, 0. Este es el valor minimax de este estado.\n",
    "\n",
    "Ahora consideremos un nivel más arriba. Juega $X$ (es decir, MAX):\n",
    "\n",
    "<img src=\"imagenes/ejemplo4.png\" width=\"550px\">\n",
    "\n",
    "En el estado que vemos más arriba, el jugador de las $X$ es quien tiene el turno, es decir, juega MAX. Él debe seleccionar la opción que le reporte el máximo de los valores minimax de sus hijos. En este caso, debe seleccionar la única opción con valor 0. Observe que si $X$ seleccionara cualquier otra opción (las cuales tienen valor $-1$), entonces MIN ganaría (suponiendo que MIN juega de la mejor manera posible). Esto es, MAX busca la acción cuyo resultado tenga el mayor valor minimax, por lo que decide poner una $X$ en la casilla de abajo a la izquierda para bloquear el triqui de $O$. Esta acción resulta en el estado de la derecha. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo código para minimax <a class=\"anchor\" id=\"codigo\"></a>\n",
    "\n",
    "([Volver a minimax](#minimax))\n",
    "\n",
    "El siguiente es el pseudo código del algoritmo de decisión minimax, el cual genera, mediante una metodología depth-first, el árbol de confrontación para un estado `s` en el cual le corresponde el turno a MAX:\n",
    "\n",
    "<img src=\"imagenes/busqueda-minimax.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Hay dos errores en el pseudocódigo:\n",
    "\n",
    "1. La función Minimax-search no necesita la primera línea para guardar la información del agente al que le corresponde el turno.\n",
    "2. Las funciones Max-value y Min-value sí necesitan incluir una primera línea con la información del agente al que le corresponde el turno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "Implemente el anterior pseudocódigo para la función `minimax_search`. Compruebe que la respuesta de $X$ para el siguiente tablero debe ser bloquear el triqui de $O$ en la casilla $(0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minimax_search(juego, estado):\n",
    "    pass  \n",
    "\n",
    "def max_value(juego, estado):\n",
    "    pass\n",
    "\n",
    "def min_value(juego, estado):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "s = tri.resultado(s, (1,1)) # Juega X\n",
    "s = tri.resultado(s, (0,2)) # Juega O\n",
    "s = tri.resultado(s, (1,2)) # Juega X\n",
    "s = tri.resultado(s, (1,0)) # Juega O\n",
    "s = tri.resultado(s, (2,1)) # Juega X\n",
    "s = tri.resultado(s, (0,1)) # Juega O\n",
    "tri.render(s)\n",
    "print(\"Computador juega en:\", minimax_search(tri, s)) \n",
    "# La respuesta debe ser (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "Compruebe la correctitud de su algoritmo mediante los siguientes tableros de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer tablero de prueba\n",
    "s1 = tri.resultado(tri.estado_inicial, (1,1))  # Juega X\n",
    "s1 = tri.resultado(s1, (0,1)) # Juega O\n",
    "s1 = tri.resultado(s1, (2,1)) # Juega X\n",
    "s1 = tri.resultado(s1, (2,2)) # Juega O\n",
    "s1 = tri.resultado(s1, (1,0)) # Juega X\n",
    "s1 = tri.resultado(s1, (2,0)) # Juega O\n",
    "tri.render(s1)\n",
    "print(\"Computador juega en:\", minimax_search(tri, s1)) \n",
    "# La respuesta debe ser (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo tablero de prueba\n",
    "s1 = tri.resultado(tri.estado_inicial, (2,1)) # Juega X\n",
    "s1 = tri.resultado(s1, (0,1)) # Juega O\n",
    "s1 = tri.resultado(s1, (1,2)) # Juega X\n",
    "s1 = tri.resultado(s1, (1,0)) # Juega O\n",
    "s1 = tri.resultado(s1, (1,1)) # Juega X\n",
    "s1 = tri.resultado(s1, (0,2)) # Juega O\n",
    "tri.render(s1)\n",
    "print(\"Computador juega en:\", minimax_search(tri, s1)) \n",
    "# La respuesta debe ser (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tercer tablero de prueba\n",
    "s1 = tri.resultado(tri.estado_inicial, (1,1)) # Juega X\n",
    "s1 = tri.resultado(s1, (0,1)) # Juega O\n",
    "s1 = tri.resultado(s1, (0,0)) # Juega X\n",
    "s1 = tri.resultado(s1, (2,2)) # Juega O\n",
    "tri.render(s1)\n",
    "print(\"Computador juega en:\", minimax_search(tri, s1)) \n",
    "# La respuesta debe ser (1, 0) o (2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que la clase `Triqui` tiene el método `player`, el cual recibe un estado `s` y devuelve el jugador al que le corresponde el turno en `s`. La codificación es que si `player(s)` es 1, le corresponde el turno a $O$; y si es 2, le corresponde el turno a $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "s = tri.resultado(s, (1,1)) # Juega X\n",
    "s = tri.resultado(s, (0,2)) # Juega O\n",
    "if tri.player(s)==2:\n",
    "    print('Le corresponde el turno a X')\n",
    "elif tri.player(s)==1:\n",
    "    print('Le corresponde el turno a O')   \n",
    "tri.render(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "Adapte la función `minimax_search` para que la decisión que se tome sea sensible al jugador al que le corresponde el turno. De esta manera, si juegan las $X$, se busca maximizar el valor, pero si juegan las $O$, se busca minimizar el valor. Confirme la correctitud de su respuesta con el cuarto tablero de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_search(juego, estado):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuarto tablero de prueba\n",
    "s1 = tri.resultado(tri.estado_inicial, (1,1)) # Juega X\n",
    "s1 = tri.resultado(s1, (0,0)) # Juega O\n",
    "s1 = tri.resultado(s1, (2,0)) # Juega X\n",
    "tri.render(s1)\n",
    "print(\"Computador juega en:\", minimax_search(tri, s1)) \n",
    "# La respuesta debe ser (0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando el engine <a class=\"anchor\" id=\"engine\"></a>\n",
    "\n",
    "([Volver a minimax](#minimax))\n",
    "\n",
    "¡Ya podemos jugar contra el computador! Intente ganarle al engine que acabamos de construir mediante el análisis minimax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corra esta celda para comenzar un juego nuevo\n",
    "tri = Triqui()\n",
    "s = tri.estado_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzca aquí su jugada, corra la celda y espere\n",
    "# la jugada de O. Luego, vuelva a cambiar aquí mismo \n",
    "# su jugada y corra la celda de nuevo, etc.\n",
    "a = (0,1)\n",
    "\n",
    "assert(a in tri.acciones(s)), 'Acción no permitida. Intente de nuevo.'\n",
    "\n",
    "###################\n",
    "s = tri.resultado(s, a)\n",
    "#clear_output(wait=True)\n",
    "tri.render(s)\n",
    "plt.show()\n",
    "\n",
    "# Computador responde\n",
    "if not tri.es_terminal(s):\n",
    "    a = minimax_search(tri, s)\n",
    "    s = tri.resultado(s, a)\n",
    "    sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    tri.render(s)\n",
    "    plt.show()\n",
    "    if tri.es_terminal(s):\n",
    "        print('Juego terminado. ¡Gana O!')\n",
    "else:\n",
    "    jugador = tri.player(s)\n",
    "    if tri.utilidad(s, jugador)==0:\n",
    "        print('Juego terminado. ¡Empate!')\n",
    "    else:\n",
    "        print('Juego terminado. ¡Gana X!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo minimax realiza una exploración completa del espacio de estados mediante una metodología depth-first. Si la profundidad máxima del árbol fuera $m$ y en cada estado hubiera $b$ acciones posibles, entonces la complejidad de tiempo del algoritmo es $O(b^m)$. Como se están generando todos los hijos de cada nodo al mismo tiempo, la complejidad de espacio (memoria) es de $O(b^m)$.\n",
    "\n",
    "La complejidad exponencial de este algoritmo lo hace ineficiente para muchos juegos complejos. Una manera de prevenir que el árbol de estados crezca tan rápido es dejando de explorar opciones que no parecen viables. Una de estas alternativas se llama el algoritmo de poda alfa-beta, que veremos a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de poda alfa-beta <a class=\"anchor\" id=\"alfa-beta\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Para explicar el funcionamiento de la poda alfa-beta consideremos un ejemplo más bien simple. Supongamos un juego que define el siguiente árbol, con la utilidad de los estados terminales ya puestos sobre la figura debajo de cada hoja, y los nombres de las acciones posibles etiquetando cada arista:\n",
    "\n",
    "<img src=\"imagenes/alfa-beta.png\" width=\"350px\">\n",
    "\n",
    "Este árbol lo hemos puesto para ilustrar el comportamiento del juego. No obstante, es muy importante observar que la idea del algoritmo es evitar tener que construir todo el árbol, deteniendo la exploración del mismo cuando sabemos que una acción no es conveniente para el jugador respectivo. Vamos a ver cómo se logra esto. \n",
    "\n",
    "La creación del árbol de confrontación se hace explorando los hijos de cada estado. Durante esta exploración se llevará un registro del valor máximo de las acciones hasta ahora. Este es el valor alfa. También se llevará un registro del valor mínimo (beta). Veamos en nuestro ejemplo para qué llevamos este registro de alfa (el funcionamiento de beta es análogo intercambiando jugadores). \n",
    "\n",
    "El algoritmo comienza a explorar el árbol primero en profundidad, inicializado con un valor de alfa de $-\\infty$ y de beta de $\\infty$. Estamos asumiendo que en la raíz (estado A) le corresponde el turno a MAX:\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-1.png\" width=\"350px\">\n",
    "\n",
    "Una vez se obtiene el valor minimax para el primer hijo explorado (en este caso B), se puede asignar un valor a alfa (en este caso, 3):\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-2.png\" width=\"350px\">\n",
    "\n",
    "Este valor de alfa se usa en la exploración del siguiente hijo, en este caso el estado C. Esto es, en la expansión de los hijos de C (subexpansiones) se utiliza el valor de alfa = 3. Aquí observamos que la primera subexpansión representa una utilidad de 2, que es inferior a alfa. Este es el criterio para detener la exploración de C.\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-3.png\" width=\"350px\">\n",
    "\n",
    "La razón de esta detención debe ser clara. Para establecer el valor minimax del estado C, MIN buscará una opción con mínimo valor minimax. Este valor será menor o igual a 2. Esto permite deducir que en el estado A, MAX no tomará la acción que lo lleve al estado C, toda vez que el valor minimax de una acción ya explorada le reporta un mejor valor (a saber, la acción que lo lleva al estado B). Es por esto que no tiene sentido seguir explorando el estado C y toda esta rama puede podarse. \n",
    "\n",
    "### Pseudo código de la poda alfa-beta\n",
    "\n",
    "El siguiente es el pseudo código para el algoritmo de poda alfa-beta:\n",
    "\n",
    "<img src=\"imagenes/poda-alfa-beta.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Los mismos errores que en el Minimax-search ocurren en este pseudocódigo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3)) ([Próximo ejercicio](#ej5))\n",
    "\n",
    "Implemente un código python `alpha_beta_search` con la poda alfa-beta y corra el algoritmo desde la raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from juegos import Triqui\n",
    "import numpy as np\n",
    "\n",
    "def alpha_beta_search(juego, estado):\n",
    "    pass\n",
    "\n",
    "def max_value_alfa_beta(juego, estado, alfa, beta):\n",
    "    pass\n",
    "\n",
    "def min_value_alfa_beta(juego, estado, alfa, beta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "alpha_beta_search(tri, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej5\"></a>**Ejercicio 5:** \n",
    "\n",
    "([Anterior ejercicio](#ej4)) ([Próximo ejercicio](#ej6))\n",
    "\n",
    "Adapte la función `alpha_beta_search` para que la decisión que se tome sea sensible al jugador al que le corresponde el turno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparación de tiempos**\n",
    "\n",
    "Vamos a revisar de manera empírica, mediante tiempos de CPU, qué tanto más eficiente es el algoritmo de poda alfa-beta respecto al algoritmo original minimax, para el caso que nos ocupa de implementar el triqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la lista de funciones\n",
    "funs = [\n",
    "    lambda s: minimax_search(tri, s),\n",
    "    lambda s: alpha_beta_search(tri, s),\n",
    "]\n",
    "# Creamos la lista de nombres de las funciones\n",
    "nombres = [\n",
    "    'Minmax', \n",
    "    'Alfa-Beta', \n",
    "]\n",
    "\n",
    "# Inicializamos el juego\n",
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "# Definimos una lista fija de jugadas para ambas funciones\n",
    "jugadas = [(0, 0), (1, 1), (0, 1)]\n",
    "# Creamos una lista auxiliar de los datos por jugada\n",
    "lista_dfs = []\n",
    "# Iteramos por jugadas\n",
    "for i, a in enumerate(jugadas):\n",
    "    data = compara_funciones(\n",
    "        lista_funs=funs, \n",
    "        lista_args=[[s], [s]], \n",
    "        lista_nombres=nombres, \n",
    "        num_it=5\n",
    "    )\n",
    "    data['No. de jugada'] = i\n",
    "    lista_dfs.append(data)\n",
    "    s = deepcopy(tri.resultado(s, a))\n",
    "data = pd.concat(lista_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficando\n",
    "fig, ax = plt.subplots(1,1, figsize=(3*len(funs),3), tight_layout=True)\n",
    "sns.lineplot(data=data, hue='Función', x='No. de jugada', y='tiempo_CPU', ax=ax)\n",
    "ax.set_ylabel('Tiempo CPU para\\ndecidir nueva jugada')\n",
    "plt.savefig('minimax_vs_poda.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, en este caso, la poda alfa-beta hace que la búsqueda de una decisión sea muy eficiente, comparada con la búsqueda del algoritmo minimax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de desempeño\n",
    "\n",
    "Vamos a comparar el algoritmo contra un jugador que toma acciones de manera aleatoria. Primero veámos una presentación del agente de manera visual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "tri = Triqui()\n",
    "# Create other player\n",
    "algorithm = lambda s: alpha_beta_search(tri, s)\n",
    "other_player = PlfromAlgo(algorithm)\n",
    "# Create environment\n",
    "env = EnvfromGameAndPl2(tri, other_player)\n",
    "# Create player\n",
    "choices = list(product([0,1,2], repeat=2))\n",
    "player_random = Random(choices=choices)\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=player_random,\\\n",
    "        model_name='Random',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "#episodio.simulate(verbose=4)\n",
    "episodio.sleep_time = 1\n",
    "episodio.renderize()\n",
    "s = episodio.environment.state\n",
    "print(episodio.environment.game.give_result(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí juegan un alfa-beta contra otro alfa-beta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "tri = Triqui()\n",
    "# Create players\n",
    "algorithm = lambda s: alpha_beta_search(tri, s)\n",
    "player_alpha_beta = PlfromAlgo(algorithm)\n",
    "other_player = PlfromAlgo(algorithm)\n",
    "# Create and load environment\n",
    "env = EnvfromGameAndPl2(tri, other_player)\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=player_alpha_beta,\\\n",
    "        model_name='alfa-beta',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "#episodio.simulate(verbose=4)\n",
    "episodio.sleep_time = 1\n",
    "episodio.renderize()\n",
    "s = episodio.environment.state\n",
    "print(episodio.environment.game.give_result(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el siguiente código vemos una comparación del desempeño del agente alfa-beta, jugando como segundo jugador (es decir, las \"O\" que juegan de segundo) contra un agente aleatorio y otro agente alfa-beta. Corremos un torneo con 10 episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp = Experiment(num_rounds=300, \\\n",
    "                       num_episodes=10)\n",
    "# Inicialize game\n",
    "tri = Triqui()\n",
    "# Set other player as alfa-beta\n",
    "algorithm = lambda s: alpha_beta_search(tri, s)\n",
    "other_player = PlfromAlgo(algorithm)\n",
    "# Create and load environment\n",
    "env = EnvfromGameAndPl2(tri, other_player)\n",
    "exp.load_env(env, \"triqui\")\n",
    "# Create players\n",
    "# Random\n",
    "choices = list(product([0,1,2], repeat=2))\n",
    "player_random = Random(choices=choices)\n",
    "# Alfa-Beta\n",
    "player_alpha_beta = PlfromAlgo(algorithm)\n",
    "# Run the experiment\n",
    "agents_list = [player_random, player_alpha_beta]\n",
    "names = ['Random', 'Alfa-Beta']\n",
    "# Run the experiment\n",
    "exp.run_experiment(agents=agents_list, \\\n",
    "                   names=names, \\\n",
    "                   measures=['histogram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son los de esperar. El agente alfa-beta le gana la gran mayoría de las veces al random, pero siempre empata contra otro alfa-beta. En cualquier caso, el agoritmo alfa-beta nunca pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de evaluación <a class=\"anchor\" id=\"feval\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a ver ahora un ejemplo muy bonito en el que se requiere usar funciones de evaluación para darle un valor a un estado dado. Observe que los dos algoritmos anteriores atribuyen valor minimax a los estados a partir de la utilidad de los estados terminales. Estos algoritmos toman esta utilidad y la 'suben' por los estados hasta la raíz. Pero, ¿qué pasa si es muy ineficiente llegar hasta el estado final para obtener una utilidad? Esto ocurre en el juego del ajedrez, en donde la explosión de estados es exponencial. Por ejemplo, expandir un tablero tres jugadas hacia adelante implica considerar alrededor de 726 millones de estados.\n",
    "\n",
    "Para evitar tener que bajar hasta los estados terminales, se puede usar una función `is_cutoff(s)`, la cual utiliza un criterio de detención para la expansión de estados. Esta función reemplazará el criterio `es_terminal` en los algoritmos minimax y poda alfa-beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cutoff(self, board, d):\n",
    "    if self.es_terminal(board):\n",
    "        return True\n",
    "    elif d >= self.max_lim:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se 'dispara' la condición `is_cutoff` y el estado no es terminal, debemos retornar un valor. Este valor estará dado por una función de evaluación, el cual debe aproximar qué tan bueno es un estado. Si el estado es mejor para MAX, la función de evaluación debe retornar valores positivos grandes; si es mejor para MIN, debe retornar valores negativos grandes. Veamos esto en el ejemplo del ajedrez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Torre y rey contra rey solitario**\n",
    "\n",
    "Uno de los finales que todo principiante en ajedrez debe aprender es el de hacer mate mediante torre y rey contra rey solitario. Hemos creado una implementación de este juego en la clase `ReyTorreRey` en el módulo `juegos` usando la librería [python-chess](https://python-chess.readthedocs.io/en/latest/). Note que las acciones usan la [notación algebráica estándar](https://es.wikipedia.org/wiki/Notaci%C3%B3n_algebraica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tablero se puede hacer mate en una jugada. El mate se obtiene después de Ra3. Podemos incluir cada jugada de manera manual mediante el método `jugada_manual` en lugar de `resultado`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = roo.jugada_manual(s, 'Ra3')\n",
    "print('Mate?', roo.es_terminal(s))\n",
    "print('Utilidad:', roo.utilidad(s, roo.player(s)))\n",
    "roo.render(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, a pesar de que el mate está tan cerca en este estado, un árbol de confrontación tendrá que crear 16 nodos en el primer nivel, esto es, 12 jugadas posibles para la torre y 4 para el rey. El movimiento del rey contrincante genera menos nodos, a lo sumo 8, pero cada jugada adicional del blanco requerirá una cantidiad cercana a 18 nodos. Esta metodología, incluso con un alfa-beta, se quedará atascada en una progresión en la cual el rey blanco se va alejando cada vez más del rey negro. \n",
    "\n",
    "Para evitar este crecimiento del árbol en profundidad, usamos el criterio `is_cutoff`, junto con una función de evaluación para estimar una utilidad en el estado truncado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente con funciones de evaluación\n",
    "\n",
    "Observe que el anterior tablero muestra tres condiciones indispensables para dar mate con torre y rey:\n",
    "\n",
    "1. El rey blanco debe cortar los movimientos del rey negro.\n",
    "2. El rey negro debe estar en uno de los bordes del tablero.\n",
    "3. La torre debe atacar al rey sin poder ser comida por este.\n",
    "\n",
    "La primera función de evaluación que consideraremos mide qué tan cerca a la oposición están los dos reyes. Cada vez que se alcance un cutoff en un estado `s`, la función de evaluación nos dará el valor máximo entre la distancia vertical y la horizontal de los reyes. Esto hará que el rey negro se lance como un chita contra el rey blanco.\n",
    "\n",
    "Hemos implementado esta función en el agente `ChessFinalist1` del módulo `agents` y podemos visualizarlo mediante la clase `Episode` que ya conocíamos del primer notebook.\n",
    "\n",
    "**Nota:** Observe que estamos creando un entorno uniendo el juego y un jugador automático como segundo jugador. Esto se hace mediante la clase `EnvfromGameAndPl2` del módulo `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=6)\n",
    "# Create other player\n",
    "other_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='negras', \n",
    "    pesos=[0,1,0,1],\n",
    "    max_lim=3)\n",
    "# Create environment\n",
    "env = EnvfromGameAndPl2(roo, other_player)\n",
    "# Create player\n",
    "chess_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='blancas',\n",
    "    max_lim=3,\n",
    "    pesos=[1,0,0,0])\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=chess_player,\\\n",
    "        model_name='Borde',\\\n",
    "        num_rounds=15)\n",
    "# Visualize\n",
    "# episodio.simulate(verbose=4)\n",
    "episodio.sleep_time = 0.3\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rey y la torre blancos no logran dar mate al rey negro. Este último se queda en el medio, pues eso es lo que le dice su función de evaluación: cuánto más estés en el centro, menor será el puntaje (las negras son \"min\"):\n",
    "\n",
    "<img src=\"imagenes/rincon.png\" width=\"300px\">\n",
    "\n",
    "Observe que `ChessFinalist1` tiene implementadas cuatro funciones de evaluación:\n",
    "\n",
    "   1. Oposición: Que da un valor mayor cuanto más cerca estén los reyes de la oposición.\n",
    "   2. Material: Que da un valor positivo a las piezas blancas y uno negativo a las piezas negras y devuelve la suma.\n",
    "   3. Rectángulo: Sobre el cual volveremos más adelante.\n",
    "   4. Rey negro en el borde: Que implementa la evaluación de la figura anterior, pasada por una función exponencial.\n",
    "    \n",
    "Es posible combinar las funciones de evaluación para obtener una evaluación unificada, que trata de tener en cuenta todos los factores anteriores. Estos valores pueden modificarse al inicializar el agente mediante el argumento `pesos`. Observe que el rey negro fue implementado con `pesos=[0,1,0,1]`, en el cual tomamos en cuenta quedarnos en el centro y capturar la torre enemiga si es posible. Tenga en cuenta también que los pesos no son binarios y pueden tomar valores reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=5)\n",
    "# Create other player\n",
    "other_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='negras', \n",
    "    pesos=[0,1,0,1],\n",
    "    max_lim=3)\n",
    "# Create boards for sanity check\n",
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "print(\"Evaluación del rey negro en el borde:\", other_player._rey_borde(s))\n",
    "#print(\"Valor en términos de la figura:\", np.log(other_player._rey_borde(s)/10))\n",
    "roo = ReyTorreRey(tablero_inicial=6)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "print(\"Evaluación del rey negro en el borde:\", other_player._rey_borde(s))\n",
    "#print(\"Valor en términos de la figura:\", np.log(other_player._rey_borde(s)/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rectángulo**\n",
    "\n",
    "Para llevar al rey negro al borde, es necesario usar la torre. Podemos darle un mayor puntaje al estado de las blancas cuanto más arrinconado esté el rey negro por la torre blanca. El siguiente tablero muestra el valor de la función de evaluación que calcula el área disponible para el rey negro con base en la casilla en que se encuentra la torre blanca:\n",
    "\n",
    "<img src=\"imagenes/rectangulo.png\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej6\"></a>**Ejercicio 6:** \n",
    "\n",
    "([Anterior ejercicio](#ej5)) ([Próximo ejercicio](#ej7))\n",
    "\n",
    "Implemente el método `_rectangulo` para calcular el área que rodea al rey negro permitida por la torre blanca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rectangulo(self, estado):\n",
    "    # Contamos el área del rectánguno creado por la torre\n",
    "    # y que rodea al rey negro\n",
    "    fila_torre_blanca, columna_torre_blanca = self._casilla_pieza(estado, 'R')\n",
    "    fila_rey_negro, columna_rey_negro = self._casilla_pieza(estado, 'k')\n",
    "    if columna_torre_blanca == columna_rey_negro:\n",
    "        if columna_torre_blanca <= 4: \n",
    "            columna_torre_blanca -= 1\n",
    "        else:\n",
    "            columna_torre_blanca += 1\n",
    "    if fila_torre_blanca == fila_rey_negro:\n",
    "        if fila_torre_blanca <= 4: \n",
    "            fila_torre_blanca -= 1\n",
    "        else:\n",
    "            fila_torre_blanca += 1\n",
    "    # AQUÍ SU CÓDIGO\n",
    "    if fila_rey_negro < fila_torre_blanca:\n",
    "        if columna_rey_negro < columna_torre_blanca:\n",
    "            area = ...\n",
    "        else:\n",
    "            area = ...\n",
    "    else:\n",
    "        if columna_rey_negro < columna_torre_blanca:\n",
    "            area = ...\n",
    "        else:\n",
    "            area = ...\n",
    "    # HASTA AQUÍ SU CÓDIGO\n",
    "    assert(area != 0), display(estado)\n",
    "    return 64 / area\n",
    "\n",
    "\n",
    "setattr(ChessFinalist1, '_rectangulo', _rectangulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player\n",
    "chess_player = ChessFinalist1(game=roo)\n",
    "# Create game\n",
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "# Check board evaluation\n",
    "evaluacion = 64 / chess_player._rectangulo(s)\n",
    "print('Área del rectángulo:', evaluacion)\n",
    "assert(np.isclose(evaluacion, 4))\n",
    "# Create game\n",
    "roo = ReyTorreRey(tablero_inicial=5)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "# Check board evaluation\n",
    "evaluacion = 64 / chess_player._rectangulo(s)\n",
    "print('Área del rectángulo:', evaluacion)\n",
    "assert(np.isclose(evaluacion, 49))\n",
    "# Create game\n",
    "roo = ReyTorreRey(tablero_inicial=7)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "# Check board evaluation\n",
    "evaluacion = 64 / chess_player._rectangulo(s)\n",
    "print('Área del rectángulo:', evaluacion)\n",
    "assert(np.isclose(evaluacion, 35))\n",
    "# Create game\n",
    "roo = ReyTorreRey(tablero_inicial=8)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "# Check board evaluation\n",
    "evaluacion = 64 / chess_player._rectangulo(s)\n",
    "print('Área del rectángulo:', evaluacion)\n",
    "assert(np.isclose(evaluacion, 30))\n",
    "# Create game\n",
    "roo = ReyTorreRey(tablero_inicial=9)\n",
    "s = roo.estado_inicial\n",
    "roo.render(s)\n",
    "# Check board evaluation\n",
    "evaluacion = 64 / chess_player._rectangulo(s)\n",
    "print('Área del rectángulo:', evaluacion)\n",
    "assert(np.isclose(evaluacion, 49))\n",
    "print('¡Tests superados!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos al rey y torre blancos acorralar al rey negro y darle jaque mate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=5)\n",
    "# Create other player\n",
    "other_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='negras', \n",
    "    pesos=[0,1,0,1],\n",
    "    max_lim=2)\n",
    "#other_player.debug = True\n",
    "# Create environment\n",
    "env = EnvfromGameAndPl2(roo, other_player)\n",
    "# Create player\n",
    "chess_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='blancas',\n",
    "    max_lim=2,\n",
    "    pesos=[1,1,1,1])\n",
    "#chess_player.debug = True\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=chess_player,\\\n",
    "        model_name='Borde',\\\n",
    "        num_rounds=50)\n",
    "# Visualize\n",
    "#episodio.simulate(verbose=4)\n",
    "episodio.sleep_time = 0.1\n",
    "episodio.renderize()\n",
    "#s = episodio.environment.state\n",
    "#print(episodio.environment.game.give_result(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "exp = Experiment(num_rounds=20, \\\n",
    "                 num_episodes=1)\n",
    "# Create other player\n",
    "other_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='negras', \n",
    "    pesos=[0,1,0,1],\n",
    "    max_lim=3)\n",
    "# Load test suite\n",
    "exp.load_game_test_suite(other_player, \"test_suite_rey_torre_rey.json\")\n",
    "exp.test_suite = exp.test_suite[:10]\n",
    "# Create list of agents\n",
    "chess_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='blancas',\n",
    "    max_lim=3,\n",
    "    pesos=[1,1,1,1])\n",
    "agents_list = [chess_player]\n",
    "names = ['ChessFinalist']\n",
    "# Run the experiment\n",
    "exp.run_experiment(agents=agents_list, \\\n",
    "                   names=names, \\\n",
    "                   measures=['histogram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el agente logra dar mate dos de cada 10 veces cuando damos un máximo de quince jugadas. Aún hay espacio para mejorar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search <a class=\"anchor\" id=\"mcts\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Hay juegos para los cuales resulta difícil definir una función de evaluación que permitan un desempeño óptimo del agente. O bien porque no se cuenta con suficiente información de experto o bien porque el problema es reclacitrante a los intentos más sensatos de crear una buena función de evaluación. En estas situaciones, se ha encontrado muy útil hacer una exploración aleatoria de las opciones de juego en un tablero, de tal manera que la evaluación del tablero se obtiene a partir de la utilidad promedio de los resultados obtenidos de la exploración. Esta estrategia ha sido llamada Búsqueda Pura Monte Carlo. Más aún, se ha creado un algoritmo, llamado Búsqueda de Árboles Monte Carlo, el cual trate de tomar ventaja de las acciones más promisorias mediante la extensión progresiva de un árbol de búsqueda. Los recursos computacionales se invierten en su mayoría a aprovechar los nodos más prometedores, y solo un poco en la exploración. \n",
    "\n",
    "Los componentes de este algoritmo son:\n",
    "\n",
    "1. Selección\n",
    "2. Expansión\n",
    "3. Simulación\n",
    "4. Backup de valores\n",
    "\n",
    "Veremos a continuación los componentes de esta estrategia, aplicada al ejemplo del Rey y Torre contra Rey solitario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulación de juego <a class=\"anchor\" id=\"sim\"></a>\n",
    "\n",
    "([Volver a Monte Carlo Tree Search](#mcts))\n",
    "\n",
    "\n",
    "Comenzaremos con las simulaciones. Para hacer una simulación, el computador toma el tablero y selecciona una jugada posible para el jugador correspondiente, luego obtiene el tablero resultado y selecciona una jugada posible para el otro jugador, y así hasta terminar el juego. Observe que proceder de esta manera es construir un camino desde el tablero inicial hasta un estado terminal. En esta simulación no se expande un árbol, pues no se consideran todas las opciones de juego en cada paso; solamente consideramos una. En otras palabras, sólo se abre una rama del árbol (el cual es inexistente).\n",
    "\n",
    "La selección de jugadas puede hacerse de manera aleatoria, o puede utilizarse para ello una 'política de juego' o 'rollout policy', si se dispone de esta información. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=4)\n",
    "# Create player\n",
    "player = ChessFinalist2(\n",
    "    game=roo,\n",
    "    jugador='blancas', \n",
    "    max_lim=2,\n",
    "    sim_lim=20\n",
    ")\n",
    "s = roo.estado_inicial\n",
    "player.debug = False\n",
    "player.simulate_game(s, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de nodos <a class=\"anchor\" id=\"seleccion\"></a>\n",
    "\n",
    "([Volver a Monte Carlo Tree Search](#mcts))\n",
    "\n",
    "Recordemos que nuestro objetivo es ranquear las jugadas posibles dado un tablero. Para realizar esto, vamos a buscar jugadas que frecuentemente llevan a la victoria. Observe que estamos en una situación en donde no tenemos recursos computacionales para explorar todas las jugadas posibles. Así pues, nuestra exploración cubrirá sólo una porción pequeña del árbol de confrontación. Si esta exploración se hace de manera completamente aleatoria, la información resultante usualmente no resulta muy útil. Más bien, la idea es realizar la exploración de manera que se exploten las jugadas más prometedoras que se vayan encontrado, de tal manera que tengamos más evidencia sobre ellas. Para ello se tratará de balancear el dilema entre explorar nuevas opciones y explotar aquellas más prometedoras.\n",
    "\n",
    "Para guiar este proceso usaremos una estructura de árbol, en el cual cada nodo guarda un tablero y la utilidad obtenida hasta ahora por el jugador respectivo. La utilidad la obtenemos mediante la simulación que implementamos en la sección anterior. La idea es ir expandiendo los nodos más prometedores, es decir, aquellos en donde las simulaciones han mostrado mayor utilidad para el jugador respectivo. De esta manera, los recursos computacionales se enfocan en pocas combinaciones de jugadas, que se exploran más frecuentemente que otras menos prometedoras.\n",
    "\n",
    "Nuestra estructura de nodos, que es una estructura recursiva basada en nodos con los siguientes atributos:\n",
    "\n",
    "* **estado**: el tablero correspondiente al nodo.\n",
    "* **madre**: el nodo del que proviene.\n",
    "* **accion**: acción usada para obtener el nodo en cuestión desde su nodo madre.\n",
    "* **Valor**: guarda la información de la recompensa obtenida desde alguno de los sucesores del nodo, sobre el total de la exploración.\n",
    "\n",
    "La siguiente es una posible implementación de esta estructura en python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "# Create player\n",
    "player = ChessFinalist2(\n",
    "    game=roo,\n",
    "    jugador='blancas', \n",
    "    max_lim=2,\n",
    "    sim_lim=20,\n",
    "    time_lim=20,\n",
    "    beam_width=10,\n",
    ")\n",
    "s = roo.estado_inicial\n",
    "arbol = ArbolBusquedaJuego(s, \\\n",
    "                           juego=player.game, \\\n",
    "                           rollout_policy=player.play_policy, \\\n",
    "                           sim_lim=50, \\\n",
    "                           beam_width=2, \\\n",
    "                           ucb_constant=2,\n",
    "                                )\n",
    "n = arbol.raiz\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo = arbol.seleccionar_ucb()\n",
    "print(nodo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper confidence bound formula**\n",
    "\n",
    "Sea $n$ un nodo y suponga que \n",
    "\n",
    "* $U(n)$ es la utilidad acumulada de las simulaciones desde $n$ o uno de sus descendientes.\n",
    "* $N(n)$ es el número total de simulaciones realizadas desde $n$ o uno de sus descendientes.\n",
    "* $C$ es la constante que permite balancear la exploración vs la explotación.\n",
    "\n",
    "Entonces la fórmula de cota de confianza superior (UCB1) es la siguiente:\n",
    "\n",
    "$$UCB1(n)=\\frac{U(n)}{N(n)} + C\\sqrt{\\frac{\\log(N(n.madre))}{N(n)}}$$\n",
    "\n",
    "El término de la izquierda es el promedio de la utilidad, que en nuestro caso se reduce a la frecuencia relativa de victorias. El término de la derecha representa una relación entre exploración y explotación. Es inversamente proporcional a la cantidad de veces que $n$ ha sido explorado y directamente proporcional a la cantidad de veces que $n.madre$ ha sido explorado. Es importante notar que el denominador de este término crece mucho más rápido que su numerador, así que el término tenderá hacia cero a medida que $N(n)$ crezca. Esto es, el término de la derecha tiende a desaparecer a medida que el nodo $n$ es explorado más veces.\n",
    "\n",
    "Esta fórmula puede entenderse también de la siguiente manera. Si $n$ no ha sido explorado muchas veces, el valor de $UCB1(n)$ tiende a ser igual a $C$. Para nodos que han sido explorados muchas veces, el valor de $UCB1(n)$ tiende a ser igual a la utilidad promedio de $n$ (es decir, $\\frac{U(n)}{N(n)}$).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valor de UCB para un nodo poco explorado:\")\n",
    "n = 1\n",
    "m = 100\n",
    "ucb_constant = 2\n",
    "print(ucb_constant * np.sqrt(np.log(m) / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_constant = 2\n",
    "# Nodo cada vez más explorado:\n",
    "for n in range(1, 1000, 10):\n",
    "    clear_output(wait=True)\n",
    "    m = 5*n\n",
    "    print(\"Valor de UCB a medida que el nodo se explora más veces:\")\n",
    "    print(ucb_constant * np.sqrt(np.log(m) / n))\n",
    "    sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansión del árbol <a class=\"anchor\" id=\"expansion\"></a>\n",
    "\n",
    "([Volver a Monte Carlo Tree Search](#mcts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-up del valor <a class=\"anchor\" id=\"back-up\"></a>\n",
    "\n",
    "([Volver a Monte Carlo Tree Search](#mcts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "# Create player\n",
    "player = ChessFinalist2(\n",
    "    game=roo,\n",
    "    jugador='blancas', \n",
    "    max_lim=2,\n",
    "    sim_lim=20,\n",
    "    time_lim=20,\n",
    "    beam_width=10,\n",
    ")\n",
    "s = roo.estado_inicial\n",
    "player.states.append(s)\n",
    "player.show_tree = True\n",
    "player.program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acción seleccionada:', player.plan[0])\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize game\n",
    "roo = ReyTorreRey(tablero_inicial=5)\n",
    "# Create other player\n",
    "other_player = ChessFinalist1(\n",
    "    game=roo,\n",
    "    jugador='negras', \n",
    "    pesos=[0,1,0,1],\n",
    "    max_lim=2)\n",
    "#other_player.debug = True\n",
    "# Create environment\n",
    "env = EnvfromGameAndPl2(roo, other_player)\n",
    "# Create player\n",
    "chess_player = ChessFinalist2(\n",
    "    game=roo,\n",
    "    jugador='blancas', \n",
    "    max_lim=2,\n",
    "    sim_lim=10,\n",
    "    time_lim=20,\n",
    "    beam_width=7,\n",
    ")\n",
    "#chess_player.debug = True\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "        agent=chess_player,\\\n",
    "        model_name='Borde',\\\n",
    "        num_rounds=10)\n",
    "# Visualize\n",
    "#episodio.simulate(verbose=4)\n",
    "episodio.sleep_time = 0.1\n",
    "episodio.renderize()\n",
    "#s = episodio.environment.state\n",
    "#print(episodio.environment.game.give_result(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook usted aprendió \n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "* El algoritmo minimax para la toma de decisiones perfecta en un juego competitivo de 2 jugadores.\n",
    "* El algoritmo de poda alfa beta para eliminar porciones del árbol de confrontación que no influyen en la toma de decisiones.\n",
    "* Funciones de evaluación para aproximar qué tan buena es una posición para cada jugador."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
