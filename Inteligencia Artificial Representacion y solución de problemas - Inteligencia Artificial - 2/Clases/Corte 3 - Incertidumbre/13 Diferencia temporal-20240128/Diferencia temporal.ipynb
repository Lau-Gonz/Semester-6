{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp>\n",
    "                <h1 style=\"color:blue;text-align:center\">Inteligencia Artificial</h1\n",
    "            </tp>\n",
    "            <tp>\n",
    "                <p style=\"font-size:150%;text-align:center\">Métodos de Diferencia Temporal</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "# Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook veremos una manera de implementar los métodos tabulares de Diferencia Temporal, los cuales usaremos para resolver algunos ambientes de tarea. Realizaremos múltiples pruebas para evaluar las bondades de cada algoritmo sobre distintos entornos.\n",
    "\n",
    "Este notebook está basado en las presentación de Sanghi (2021), capítulo 5 y sus [notebooks](https://github.com/Apress/deep-reinforcement-learning-python); y de Sutton R., & Barto, A., (2018), capítulo 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir al ejercicio 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias\n",
    "\n",
    "Al iniciar el notebook o reiniciar el kerner se pueden cargar todas las dependencias de este notebook corriendo las siguientes celdas. Este también es el lugar para instalar las dependencias que podrían hacer falta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En linux o mac\n",
    "#!pip3 install -r requirements.txt\n",
    "#!pip3 install gymnasium[toy-text]\n",
    "\n",
    "# En windows\n",
    "#!python -m pip install -r requirements.txt\n",
    "#!python -m pip installgymnasium[toy-text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Del notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambientes import GridworldEnv, CliffworldEnv\n",
    "from agents import Agent\n",
    "from algoritmos import *\n",
    "from utils import Episode, Experiment\n",
    "from plot_utils import PlotGridValues, Plot\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "# Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "* [Evaluación de una política](#eval)\n",
    "* [Métodos de control](#control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de una política <a class=\"anchor\" id=\"eval\"></a>\n",
    "    \n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Recordemos que lo primero que hacemos para resolver un ambiente de tarea es considerar el problema de la evaluación. Aquí lo que queremos es determinar el valor de un estado dada una política. En otras palabras, queremos estimar el valor esperado de la recompensa total descontada si seguimos una política a partir de un estado $s$. Esto para cada uno de los estados $s$ del ambiente de tarea. Examinaremos el método de diferencia temporal para atacar el problema y usaremos como ejemplo el entorno del GridWorld.\n",
    "\n",
    "Comenzamos definiendo una política sobre un mundo de rejilla de tamaño $4\\times 4$ (observe que esta es la política que trabajamos en el notebook anterior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "policy = ([env.NORTH] + [env.EAST]*(shape[0] - 2) + [env.SOUTH]) * shape[1]\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de estado ya los conocemos. Usaremos el método de programación dinámica para obtenerlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_eval(env, policy)\n",
    "pp.plot_policy_and_values(policy, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el método de evaluación de diferencia temporal no requerimos esperar a que el episodio entero termine antes de actualizar nuestros estimados de valor para los estados visitados. En lugar de ello, en cada ronda estamos actualizando el estimado del valor del estado que acabamos de visitar, usando un bootstrap con los valores en memoria. El pesudocódigo del algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/td_evaluationb.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "Implemente las líneas 5 a 8 del pseudocódigo anterior en el codigo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_0_evaluation(env, policy:np.array, alfa:float=0.1, gama:float=1, max_iter:int=500, max_steps=1000, V:np.array=None) -> np.array:\n",
    "    '''\n",
    "    Método de diferencia temporal para estimar el valor de los \n",
    "    estados de un MDP generando una muestra de episodios con base en una política dada. \n",
    "    Input:\n",
    "        - env, un ambiente con atributos nA, nS, shape \n",
    "               y métodos reset(), step()\n",
    "        - policy, una política determinista, policy[state] = action\n",
    "        - alfa, real con el parámetro de step-size\n",
    "        - gama, con el parámetro de factor de descuento\n",
    "        - max_iter, entero con la cantidad máxima de episodios\n",
    "        - max_steps, entero con la cantidad máxima de pasos\n",
    "        - Opcional: V, un np.array que por cada s devuelve su valor estimado\n",
    "    Output:\n",
    "        - V, un np.array que por cada s devuelve su valor estimado\n",
    "    '''\n",
    "    if V is None:\n",
    "        V = np.zeros(env.nS)\n",
    "    for _ in range(max_iter):\n",
    "        state = np.random.randint(env.nS)\n",
    "        env.state = state\n",
    "        done = False\n",
    "        counter = 0\n",
    "        while not done:\n",
    "            pass\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            \n",
    "            # HASTA AQUÍ SU CÓDIGO\n",
    "            counter += 1\n",
    "            if counter > max_steps:\n",
    "                break\n",
    "    return V \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compruebe su respuesta corriendo la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "V = td_0_evaluation(env, policy)\n",
    "VV = np.flipud(np.reshape(V, shape))\n",
    "test = np.array([[ 0.,         -4.36081499, -3.97308306, -2.99943748],\n",
    "                 [-0.99993144, -3.77424704, -2.99432731, -1.99999999],\n",
    "                 [-1.99314462, -2.7058518,  -1.99561459, -1.   ],\n",
    "                 [-2.85262296, -1.7706891, -0.99695675,  0.   ]])\n",
    "assert(np.all(np.isclose(VV, test)))\n",
    "print('¡Correcto!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de control <a class=\"anchor\" id=\"control\"></a>\n",
    "    \n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Resolver un entorno consiste en encontrar la política óptima que debe seguir el agente para maximizar su utilidad. En el mundo de la rejilla, la política óptima es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "policy = value_iteration(env, discount_factor=1, theta=0.01, verbose=0)\n",
    "print('Política óptima:')\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder encontrar la política óptima, el método de Diferencia Temporal guarda en memoria un estimado del valor estado-acción para la política, pero cada vez que cambian su estimado, también mejora la política mediante una $\\epsilon$ mejora. \n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(a|s) = \\begin{cases}\n",
    "1-\\epsilon &\\text{si }a=\\text{arg}\\!\\max_{a'} Q_{\\pi_k}(s,a') \\cr\n",
    "\\frac{\\epsilon}{\\#\\text{acciones}-1} &\\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La clase Episode\n",
    "\n",
    "Para correr las simulaciones de manera ordenada y sencilla, hemos implementado la clase `Episode`, que se encuentran en el módulo `utils.py`. Veamos un algoritmo aleatorio actuando en el entorno del mundo de la rejilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,3)\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "# Create episode\n",
    "episode = Episode(environment=env, \\\n",
    "                  env_name='GW', \\\n",
    "                  agent=agent, \\\n",
    "                  model_name='Random', \\\n",
    "                  num_rounds=3, \\\n",
    "                )\n",
    "# Run and show information\n",
    "df = episode.run(verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que hemos corrido el método `run()`, el cual corre solo un episodio, con la opción verbose = 4. Esta opción hace que el método imprima la información de cada ronda del episodio. Cuando no deseemos dicha información, podemos poner verbose = 0 (u omitir este argumento del todo, pues es opcional con valor por defecto 0) para evitar demoras innecesarias en la obtención de los datos.\n",
    "\n",
    "Veamos ahora una presentación gráfica de la evolución de la recompensa total por varios episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "# Create episode\n",
    "episode = Episode(environment=env, \\\n",
    "                  env_name='GW', \\\n",
    "                  agent=agent, \\\n",
    "                  model_name='Random', \\\n",
    "                  num_rounds=100\n",
    "                )\n",
    "# Train agent\n",
    "df = episode.simulate(num_episodes=50, verbose=0)\n",
    "# Plot rewards\n",
    "Plot(df).plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vemos la evolución de la recompensa por 50 episodios, cada uno de máximo 100 rondas. Para ello hemos usado el método `simulate()`.\n",
    "\n",
    "Podemos inspeccionar la política del agente al visualizar su atributo `policy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = episode.agent.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "print('Política del agente:')\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La clase Experiment**\n",
    "\n",
    "Una manera de usar la clase `Experiment` es para testear el agente ya entrenado y sin que este tenga que hacer exploración. Podemos usar el método `run_experiment()` sobre el agente y dibujar un histograma de la recompensa total por episodio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(environment=env,\\\n",
    "                        env_name='GW', \\\n",
    "                        num_rounds=10, \\\n",
    "                        num_episodes=10, \\\n",
    "                        num_simulations=10)\n",
    "# Test agent\n",
    "agents = experiment.run_experiment(agents=[agent],\\\n",
    "                                  names=['Random'], \\\n",
    "                                  measures=['hist_reward'], \\\n",
    "                                  learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `Experiment` tiene varias funcionalidades adicionales que exploraremos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference control <a class=\"anchor\" id=\"TDcontrol\"></a>\n",
    "    \n",
    "([Volver a Control](#control))\n",
    "\n",
    "Vamos a implementar dos agentes. Uno con la regla de aprendizaje SARSA y otro con la regla Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "Implemente el agente SARSA de acuerdo al pseudo código del agente SARSA:\n",
    "\n",
    "<img src=\"./imagenes/sarsa_agent.png\" width=\"500\"/>\n",
    "\n",
    "Use la siguiente celda para implementar el agente. El énfasis es en implementar la linea 5 encontrando el update mediante bootstrapping (estimate), el error de diferencia temporal (delta) y la actualización del valor previo al moverlo en dirección de delta una fracción alfa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Agent) :\n",
    "    '''\n",
    "    Implements a SARSA learning rule.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, parameters:dict):\n",
    "        super().__init__(parameters)\n",
    "        self.alpha = self.parameters['alpha']\n",
    "        self.debug = False\n",
    "   \n",
    "    def update(self, next_state, reward, done):\n",
    "        '''\n",
    "        Agent updates its model.\n",
    "        '''\n",
    "        # obtain previous state\n",
    "        state = self.states[-1]\n",
    "        # obtain previous action\n",
    "        action = self.actions[-1]\n",
    "        # Get next_action\n",
    "        next_action = self.make_decision()\n",
    "        # Find bootstrap\n",
    "        estimate = ... # recompensa más descuento por valor del siguiente estado\n",
    "        # Obtain delta\n",
    "        delta = ... # Diferencia temporal: estimado menos valor del estado actual\n",
    "        # Update Q value\n",
    "        prev_Q = self.Q[state, action]\n",
    "        self.Q[state, action] = ... # Actualizar en la dirección de delta por una fracción alfa\n",
    "        # Update policy\n",
    "        self.update_policy(state)\n",
    "        if self.debug:\n",
    "            print('')\n",
    "            print(dash_line)\n",
    "            print(f'Learning log:')\n",
    "            print(f'state:{state}')\n",
    "            print(f'action:{action}')\n",
    "            print(f'reward:{reward}')\n",
    "            print(f'estimate:{estimate}')\n",
    "            print(f'Previous Q:{prev_Q}')\n",
    "            print(f'delta:{delta}')\n",
    "            print(f'New Q:{self.Q[state, action]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corra la siguiente celda para verificar su implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3,4)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "    'alpha': 0.1, \\\n",
    "}\n",
    "agent_SARSA = SARSA(parameters=parameters)\n",
    "test = test_sarsa(agent_SARSA)\n",
    "if test:\n",
    "    cprint(colored('¡Test superado!', 'green'))\n",
    "else:\n",
    "    cprint(colored('¡Implementación incorrecta!', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "Implemente el agente con la regla de aprendizaje Q-learning, de acuerdo al siguiente pseudocódigo:\n",
    "\n",
    "<img src=\"./imagenes/q_learning_agent.png\" width=\"450\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning(Agent) :\n",
    "    '''\n",
    "    Implements a Q-learning rule.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, parameters:dict):\n",
    "        super().__init__(parameters)\n",
    "        self.alpha = self.parameters['alpha']\n",
    "        self.debug = False\n",
    "   \n",
    "    def update(self, next_state, reward, done):\n",
    "        '''\n",
    "        Agent updates its model.\n",
    "        '''\n",
    "        # obtain previous state\n",
    "        state = ... # Aquí estado previo\n",
    "        # obtain previous action\n",
    "        action = self.actions[-1]\n",
    "        # Find bootstrap\n",
    "        maxQ = self.max_Q(next_state) \n",
    "        estimate = ... # Calcula el estimado\n",
    "        # Obtain delta\n",
    "        delta = ... # Calcula el delta\n",
    "        # Update Q value\n",
    "        prev_Q = self.Q[state, action]\n",
    "        self.Q[state, action] = ... # Actualiza el valor\n",
    "        # Update policy\n",
    "        self.update_policy(...) # Actualizar la política en el estado        \n",
    "        if self.debug:\n",
    "            print('')\n",
    "            print(dash_line)\n",
    "            print(f'Learning log:')\n",
    "            print(f'state:{state}')\n",
    "            print(f'action:{action}')\n",
    "            print(f'reward:{reward}')\n",
    "            print(f'estimate:{estimate}')\n",
    "            print(f'Previous Q:{prev_Q}')\n",
    "            print(f'delta:{delta}')\n",
    "            print(f'New Q:{self.Q[state, action]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corra la siguiente celda para verificar su implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3,4)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "    'alpha': 0.1, \\\n",
    "}\n",
    "agent_Q = Q_learning(parameters=parameters)\n",
    "test = test_q(agent_Q)\n",
    "if test:\n",
    "    cprint(colored('¡Test superado!', 'green'))\n",
    "else:\n",
    "    cprint(colored('¡Implementación incorrecta!', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3)) ([Próximo ejercicio](#ej5))\n",
    "\n",
    "Compare el desempeño del agente SARSA con el del agente Q-learning en el entorno de la caminata por el acantilado (implementado en la clase `CliffworldEnv` del módulo `ambientes`). Observe que este ejemplo fue discutido en clase, en el cual se mencionó que el Q-learning no toma en cuenta los deslices ocacionales de la política $\\epsilon$-avara, mientras que SARSA sí.\n",
    "\n",
    "Use las siguientes especificaciones:\n",
    "\n",
    "* número máximo de rondas: 200\n",
    "* número de episodios: 500\n",
    "* número de simulaciones: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las políticas resultantes para los agentes al correr las siguientes celdas:\n",
    "\n",
    "**Nota:** Las siguientes celdas solo funcionan después de haber realizado el ejercicio 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,4)\n",
    "env = CliffworldEnv(shape=shape)\n",
    "shape = (3,4)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "sarsa = agents[0]\n",
    "p = sarsa.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "policy = np.flipud(np.reshape(policy, shape))\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,4)\n",
    "env = CliffworldEnv(shape=shape)\n",
    "shape = (3,4)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "q_agent = agents[1]\n",
    "p = q_agent.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "policy = np.flipud(np.reshape(policy, shape))\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el proceso de aprendizaje, queremos que el agente pruebe distintos cursos de acción, de manera tal que tenga una mayor confianza en que está llegando a una política óptima. No obstante, a la hora de poner a marchar al agente en producción, queremos que el agente tenga su mejor desempeño. Para ello, necesitamos poner su parámetro $\\epsilon$ en 0. \n",
    "\n",
    "Vamos a comparar el desempeño de los agentes SARSA y Q-learning en su desempeño óptimo, sin exploración. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down exploration\n",
    "agents[0].epsilon = 0\n",
    "agents[1].epsilon = 0\n",
    "for s in range(env.nS):\n",
    "    agents[0].update_policy(s)\n",
    "    agents[1].update_policy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(environment=env,\\\n",
    "                 env_name='Cliff', \\\n",
    "                 num_rounds=100, \\\n",
    "                 num_episodes=100, \\\n",
    "                 num_simulations=1)\n",
    "# Use stored agents to run test\n",
    "experiment.run_experiment(\n",
    "                agents=agents,\\\n",
    "                names=['SARSA', 'Q_learning'], \\\n",
    "                measures=['hist_reward'],\\\n",
    "                learn=False)\n",
    "print('¡Listo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En este notebook usted aprendió\n",
    "\n",
    "* Cómo implementar la regla de evaluación de una política usando diferencia temporal.\n",
    "* Cómo implementar los métodos de mejoramiento de política SARSA y Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Shanghi, N. (2021) Deep Reinforcement Learning with Python: With PyTorch, TensorFlow and OpenAI Gym. Apress. \n",
    "\n",
    "Sutton R., & Barto, A., (2015) Reinforcement Learning: An Introduction, 2nd Edition. A Bradford Book. Series: Adaptive Computation and Machine Learning series. \n",
    "\n",
    "Winder, P., (2021) Reinforcement Learning: Industrial Applications of Intelligent Agents. O’Relly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
