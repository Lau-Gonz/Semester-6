{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp>\n",
    "                <h1 style=\"color:blue;text-align:center\">Aprendizaje por refuerzo</h1\n",
    "            </tp>\n",
    "            <tp>\n",
    "                <p style=\"font-size:150%;text-align:center\">Procesos de decisión de Markov y Programación dinámica</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "# Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook veremos una manera de implementar los ambientes de tarea de los MDP. Esta implementación sigue el formato de los entornos de [gym](https://gymnasium.farama.org). También implementaremos los algoritmos de Programación dinámica para la evaluación y mejoramiento de políticas.\n",
    "\n",
    "Este notebook está basado en las presentación de Sanghi (2021), capítulo 2 y sus [notebooks](https://github.com/Apress/deep-reinforcement-learning-python); Sutton R., & Barto, A., (2015), capítulos 3 y 4; y también Winder, P., (2021), capítulo 2 y su [notebook](https://rl-book.com/learn/mdp/code_driven_intro/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ir a ejercicio 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencias\n",
    "\n",
    "Al iniciar el notebook o reiniciar el kerner, se pueden cargar todas las dependencias de este notebook al correr las siguientes celdas. Este también es el lugar para instalar las dependencias faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En linux o mac\n",
    "#!pip3 install gymnasium[toy-text]\n",
    "#!pip3 install -r requirements.txt\n",
    "#!pip3 install moviepy\n",
    "#!pip3 install termcolor\n",
    "# En windows\n",
    "#!python -m pip install -r requirements.txt\n",
    "#!python -m pip install gymnasium[toy-text]\n",
    "#!python -m pip install moviepy\n",
    "#!python -m pip install termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from time import sleep\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Del notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tiempos import *\n",
    "from ambientes import *\n",
    "from agentes import Agent\n",
    "from utils import Episode, PlotGridValues, gym_interpreter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "# Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "1. [Evaluación de políticas](#poli-eval).\n",
    "2. [Mejoramiento de políticas](#dp).\n",
    "    1. [Policy iteration](#poli-iter).\n",
    "    2. [Value iteration](#value-iter).\n",
    "    3. [Resolviendo el Frozen Lake](#fl).\n",
    "    3. [Comparación de tiempos](#comp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de políticas <a class=\"anchor\" id=\"poli-eval\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a usar la programación dinámica para encontrar los valores de estado para una política dada. La idea central es usar la ecuación de Bellman como una regla iterativa:\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_{s'}\\left( p(s' | s,\\pi(s)) \\Bigl[ r + \\gamma v_k(s') \\Bigr] \\right)$$\n",
    "\n",
    "Esto da lugar al siguiente algoritmo:\n",
    "\n",
    "<img src=\"./imagenes/policy_evaluation.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Ejercicio 1:** \n",
    "\n",
    "([Próximo ejercicio](#ej2))\n",
    "\n",
    "Vamos a encontrar los valores de estado para el problema del [ABC](#abc). Implemente el algoritmo iterativo de evaluación de políticas y utilícelo para encontrar los valores de la política `policy`, de acuerdo a la cual el agente se mueve a la derecha en cualquier estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy evaluation\n",
    "\n",
    "def policy_eval(env, policy, discount_factor=1.0, theta=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Evalúa una política para un entorno.\n",
    "    Input:\n",
    "        - env: transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - policy: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: 0 no imprime nada, \n",
    "                   1 imprime la iteración del valor\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la función de valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def expected_value(s, V):\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        value = 0\n",
    "        a = policy[s]\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            value += ... # la fórmula de valor esperado\n",
    "            # AQUÍ TERMINA SU CÓDIGO\n",
    "        return value\n",
    "    \n",
    "    # Start with a (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # Stop if change is below a threshold\n",
    "    continuar = True\n",
    "    while continuar:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            v = ... # el valor guardado para s\n",
    "            V[s] = ... # el nuevo valor en términos del expected_value\n",
    "            delta = ... # la diferencia del nuevo valor con el valor guardado\n",
    "            # AQUÍ TERMINA SU CÓDIGO\n",
    "            if verbose:\n",
    "                print(f'V[{env.dict_states[s]}]={round(V[s], 3)}; ', end='')\n",
    "        continuar = not delta < theta\n",
    "        print('')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ABC()\n",
    "policy = [env.RIGHT, env.RIGHT, env.RIGHT]\n",
    "V = policy_eval(env, \n",
    "                policy, \n",
    "                discount_factor=0.8, \n",
    "                theta=0.01, \n",
    "                verbose=True)\n",
    "V_true = np.array([6.48379331, 9.67391051, 0.        ])\n",
    "assert(np.all([np.isclose(V[i], V_true[i]) for i in range(V_true.shape[0])]))\n",
    "print(colored('¡Test superado!', 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** El resultado de debe ser \n",
    "```\n",
    "V[A]=-1.0; V[B]=8.9; V[C]=0.0; \n",
    "V[A]=5.328; V[B]=9.612; V[C]=0.0; \n",
    "V[A]=6.347; V[B]=9.669; V[C]=0.0; \n",
    "V[A]=6.469; V[B]=9.674; V[C]=0.0; \n",
    "V[A]=6.482; V[B]=9.674; V[C]=0.0; \n",
    "V[A]=6.484; V[B]=9.674; V[C]=0.0; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Ejercicio 2:** \n",
    "\n",
    "([Anterior ejercicio](#ej1)) ([Próximo ejercicio](#ej3))\n",
    "\n",
    "Vamos a encontrar los valores de estado para el entorno del GridWorld. Usaremos la política `policy` definida en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy = [env.NORTH, env.EAST, env.EAST, env.SOUTH] * 4\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(np.reshape(policy, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3\n",
    "V = ... # aquí el código para correr policy_eval\n",
    "pp.plot_policy_and_values(np.flipud(np.reshape(policy, shape)), V=np.flipud(np.reshape(V, shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejoramiento de políticas <a class=\"anchor\" id=\"dp\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Recordemos que el propósito del RL es encontrar la acción que mejor utilidad tenga en cada estado, determinando así la política óptima para el problema. Si conocemos el modelo de un MDP, podemos ir mejorando una política paso a paso. Los dos métodos de esta sección realizan el mejoramiento de una política hasta llegar a la política óptima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration <a class=\"anchor\" id=\"poli-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "En este algoritmo se busca mejorar una política $\\pi$ en cada estado $s$, definiendo $\\pi'$ de tal manera que:\n",
    "\n",
    "$$\\pi'(s) = \\mbox{arg}\\max_a q_{\\pi}(s,a)$$\n",
    "\n",
    "Esto da lugar a una nueva política $\\pi'$. Luego, se recalculan los valores $v_{\\pi'}(s)$ usando el algoritmo de evaluación de política visto en la sección anterior. Este proceso se itera hasta converger a la política óptima:\n",
    "\n",
    "<img src=\"./imagenes/p_i.png\" width=\"350\"/>\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/policy_iteration1.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Ejercicio 3:** \n",
    "\n",
    "([Anterior ejercicio](#ej2)) ([Próximo ejercicio](#ej4))\n",
    "\n",
    "Implemente el algoritmo de policy improvement y mejore la política del ejercicio 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement\n",
    "\n",
    "def policy_iteration(env, pol, discount_factor=1.0, theta=0.01):      \n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - pol: vector de longitud env.nS que representa la política\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "        - verbose: bool to print intermediate values. \n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\"     \n",
    "    def expected_value(s, a, env, V):\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        value = 0\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # Aquí su código\n",
    "            value += ... # expected value\n",
    "            # Hasta aquí su código\n",
    "        return value\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = deepcopy(pol)\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        # Aquí su código\n",
    "        V = ... # policy evaluation\n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "            a_ = ... # action predicted by policy at state s\n",
    "            policy[s] = ... # argmax of expected values\n",
    "            if a != policy[s]:\n",
    "                policy_stable = ... # Boolean to check if policy is the same\n",
    "        # Hasta aquí su código\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "policy = ([env.NORTH] + [env.EAST]* (shape[0]-2) + [env.SOUTH]) * shape[1]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "pp.plot_policy(np.reshape(policy, shape), ax=ax[0])\n",
    "policy, V = policy_iteration(env, policy, discount_factor=1, theta=0.01)\n",
    "pp.plot_policy(np.reshape(policy, shape), V=V.reshape(shape), ax=ax[1])\n",
    "ax[0].set_title('Initial policy', fontsize='18')\n",
    "ax[1].set_title('Final policy', fontsize='18')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** La respuesta debe ser equivalente a:\n",
    "\n",
    "<img src=\"./imagenes/pol_it.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de agentes óptimos\n",
    "\n",
    "Podemos visualizar el comportamiento del agente con esta política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (4,4)\n",
    "size = 4\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\n",
    "    \"nS\": size*size,\n",
    "    \"nA\": env.nA,\n",
    "    \"gamma\":0.99,\n",
    "    \"epsilon\":0\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "stochastic_policy = {(s, a):1 if policy[s]==a else 0 for a in range(env.nA) for s in range(env.nS)}\n",
    "agent.policy = stochastic_policy\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "                   env_name='GridWorld',\\\n",
    "                   agent=agent,\\\n",
    "                   model_name='DP',\\\n",
    "                   num_rounds=15)\n",
    "# Visualize\n",
    "episodio.sleep_time = 1.5\n",
    "episodio.renderize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration <a class=\"anchor\" id=\"value-iter\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Para mejorar el desempeño del algoritmo de policy iteration, se puede truncar la evaluación de la política después de una iteración para cada estado. Además, se puede combinar, en una sola regla iterativa, el mejoramiento de la política con la evaluación truncada de la política:\n",
    "\n",
    "$$v_{k+1}(s) = \\max_{a}\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{k}(s') \\Bigr] \\right)$$\n",
    "\n",
    "Se puede demostrar que la sucesión $\\{v_k\\}$ converge a $v_*$. \n",
    "\n",
    "<img src=\"./imagenes/diagrama_value_iteration.png\" width=\"250\"/>\n",
    "\n",
    "Finalmente, para obtener la política óptima $\\pi_*$ se buscan las acciones mediante el argmax de los valores óptimos obtenidos en el proceso anterior:\n",
    "\n",
    "$$\\pi_*(s) = \\mbox{arg}\\max_a\\sum_{s'}\\left( p(s' | s, a) \\Bigl[ r + \\gamma  v_{*}(s') \\Bigr] \\right)$$\n",
    "\n",
    "El algoritmo es el siguiente:\n",
    "\n",
    "<img src=\"./imagenes/value_iteration.png\" width=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Ejercicio 4:** \n",
    "\n",
    "([Anterior ejercicio](#ej3))\n",
    "\n",
    "Implemente el algoritmo de value-iteration para encontrar la política óptima del MDP. Use su algoritmo para encontrar la política óptima del Grid World. El resultado debe ser la misma política óptima encontrada mediante el método `policy_iteration` en el ejercicio 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5\n",
    "# Value iteration\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.01):\n",
    "    \"\"\"\n",
    "    Mejoramiento de una política.\n",
    "    Input:\n",
    "        - env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        - discount_factor: Gamma discount factor.\n",
    "        - theta: Stop iteration once value function change is\n",
    "            less than theta for all states.\n",
    "    Output:\n",
    "        Vector de longitud env.nS que representa la política óptima.\n",
    "    \"\"\" \n",
    "    pass\n",
    "    def expected_value(s, a, env, V):\n",
    "        value = 0\n",
    "        # Calcular el valor esperado as per backup diagram\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            # Aquí su código\n",
    "            value += ... # expected value\n",
    "            # Hasta aquí su código\n",
    "        return value\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    continuar = True\n",
    "    while continuar:\n",
    "        # Aquí su código\n",
    "        delta = ... # initialize delta\n",
    "        for s in range(env.nS):\n",
    "            v = ... # previous stored value\n",
    "            V[s] = ... # new value is max expected value\n",
    "            delta = ... # max between previous and new value \n",
    "        continuar = not delta < theta\n",
    "    # Update policy\n",
    "    policy = [np.argmax([expected_value(s, a, env, V) for a in range(env.nA)]) for s in range(env.nS)]\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv()\n",
    "policy, V = value_iteration(env, discount_factor=1, theta=0.01)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(np.reshape(policy, shape), V=np.reshape(V, shape))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolviendo el Frozen Lake <a class=\"anchor\" id=\"fl\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Ya tenemos las herramientas para resolver el entorno del Frozen Lake. En este entorno, el agente debe caminar por el hielo para recoger el regalo que se encuentra al otro lado sin caerse en ninguno de los huecos en el hielo. El problema es que el hielo es resbaladizo y el agente puede resbalarse y moverse hacia alguno de los lados con probabilidad 1/3. Veámos cuál es la política óptima con un solo hueco en el hielo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "is_slippery = True\n",
    "size = 4\n",
    "max_rounds = 50\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               desc=[\"SFFF\", \"FFHF\", \"FFFF\", \"FFFG\"], \n",
    "               is_slippery=is_slippery, \n",
    "               render_mode='rgb_array'\n",
    "              )\n",
    "env = TimeLimit(env, max_episode_steps=max_rounds)\n",
    "env.reset()\n",
    "# Include additional information about states and actions\n",
    "dict_acciones = {3:\"⬆\", 2:\"➡\", 1:\"⬇\", 0:\"⬅\"}\n",
    "dict_states = dict(zip([s for s in range(16)], [np.unravel_index(s, (4,4)) for s in range(16)]))\n",
    "setattr(env, 'nS', 16)\n",
    "setattr(env, 'nA', 4)\n",
    "setattr(env, 'dict_acciones', dict_acciones)\n",
    "setattr(env, 'dict_states', dict_states)\n",
    "# Use value iteration to find optimal policy\n",
    "policy, V = value_iteration(env,\n",
    "                            discount_factor=0.9, \n",
    "                            theta=0.01)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "V = np.array([round(x, 2) for x in V])\n",
    "V=np.flipud(np.reshape(V, shape))\n",
    "pp.plot_policy_and_values(np.flipud(np.reshape(policy, shape)), \n",
    "                          V=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "ax[0].set_title('Environment', fontsize='18')\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].imshow(env.render())\n",
    "pp.plot_policy(np.flipud(np.reshape(policy, shape)), V=V, ax=ax[1])\n",
    "ax[1].set_title('Best policy', fontsize='18')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante observar que no hemos resuelto el problema mediante `policy_iteration`, sino mediante `value_iteration` debido a los tiempos de computación. Para ver detalles de la comparación entre los tiempos de los dos algoritmos, puede consultar la sección [Comparación de tiempos](#comp).\n",
    "\n",
    "Veámos al agente en acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "parameters = {\n",
    "    \"nS\": size*size,\n",
    "    \"nA\": env.action_space.n,\n",
    "    \"gamma\":0.99,\n",
    "    \"epsilon\":0\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "stochastic_policy = {(s, a):1 if policy[s]==a else 0 for a in range(env.nA) for s in range(env.nS)}\n",
    "agent.policy = stochastic_policy\n",
    "# Create episode\n",
    "episodio = Episode(environment=env,\\\n",
    "                   env_name='FrozenLake',\\\n",
    "                   agent=agent,\\\n",
    "                   model_name='Random',\\\n",
    "                   num_rounds=max_rounds, \\\n",
    "                   state_interpreter=gym_interpreter1)\n",
    "# Visualize\n",
    "episodio.renderize(to_video=True, file='frozen_lake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de tiempos <a class=\"anchor\" id=\"comp\"></a>\n",
    "\n",
    "([Volver a Mejoramiento](#dp))\n",
    "\n",
    "Vamos a hacer el estudio empírico de la complejidad de tiempos de los dos algoritmos. Correremos ambos algoritmos sobre el Grid World con tamaños (4,4) hasta (9,9). Con cada ambiente correremos 10 cada algoritmo y registraremos los tiempos de máquina usados para encontrar la política óptima. Los resultados son los siguientes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = lambda env: policy_iteration(env, get_nice_policy(env.shape))\n",
    "v_i = lambda env: value_iteration(env)\n",
    "funs = [p_i, v_i]\n",
    "nombres_funs = ['policy-iteration', 'value-iteration']\n",
    "shapes = [(n,n) for n in range(4,15)]\n",
    "lista_args = [GridworldEnv(shape) for shape in shapes]\n",
    "df = compara_entradas_funs(funs, nombres_funs, lista_args, N=10)\n",
    "sns.lineplot(x='Long_entrada',y='Tiempo',hue='Funcion',data=df)\n",
    "plt.savefig('figura.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica muestra el tiempo promedio que cada algoritmo toma para encontrar la política óptima con distintos tamaños del Grid World. A partir de la gráfica queda muy claro que el algoritmo de `value-iteration` es más eficiente que el de `policy-iteration`. Observe también que el primero no necesita una política de entrada, mientras que sí se requiere una política como argumento del `policy-iteration`. En este ejemplo se tomó una política que converge relativamente rápido, pero otras políticas toman muchísimo más tiempo en converger. Todo esto muestra las ventajas del `value-iteration`, el cual es más rápido y no requiere política de entrada.\n",
    "\n",
    "Adicionalmente, observe que la complejidad de tiempo va creciendo mucho en ambos casos, lo cual hace que sean inviables para MDPs que tienen una gran cantidad de estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En este notebook usted aprendió\n",
    "\n",
    "* Cómo implementar MDP en python usando la librería `gym` de OpenAI.\n",
    "* Cómo implementar la evaluación de una política, para obtener los valores en cada estado.\n",
    "* Cómo implementar la metodología de mejoramiento de políticas mediante policy iteration y value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Shanghi, N. (2021) Deep Reinforcement Learning with Python: With PyTorch, TensorFlow and OpenAI Gym. Apress. \n",
    "\n",
    "Sutton R., & Barto, A., (2015) Reinforcement Learning: An Introduction, 2nd Edition. A Bradford Book. Series: Adaptive Computation and Machine Learning series. \n",
    "\n",
    "Winder, P., (2021) Reinforcement Learning: Industrial Applications of Intelligent Agents. O’Relly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
