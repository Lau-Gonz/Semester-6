{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<table>\n","    <tr>\n","        <td><img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/1485763/images/9060062/Header.png\" width=\"300\"/></td>\n","        <td>&nbsp;</td>\n","        <td>\n","            <h1 style=\"font-size:200%;color:blue;text-align:center\">    <FONT COLOR=\"blue\">  Conceptos  </p>  Bag of Words   </FONT>         </h1></td>         \n","        <td>\n","            <tp><p style=\"font-size:99%;text-align:center\">PLN </p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">Pregrado MACC 2023-2</p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">Prof. Fabián Sánchez</p></tp>\n","        </td>\n","    </tr>\n","</table>"],"metadata":{"id":"RY1SVRFYpmrO"}},{"cell_type":"markdown","source":["A continuación revisaremos rápidamente algunos conceptos claves en el desarrollo de procesamiento de lenguaje natural."],"metadata":{"id":"mMK2AqI_fnMX"}},{"cell_type":"markdown","source":["<FONT SIZE=3 COLOR=\"blue\"> **Normalización** </FONT> Es una etapa en el Procesamiento de Lenguaje Natural donde se busca poner todo el texto en igualdad de condiciones, es decir, todo en el mismo tipo de letra , eliminar signos de puntuación, quitar palabras, etc. En otras palabras limpiamos el texto para aplicar algunos modelo o técnicas al mismo.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Tokenización** </FONT>: Es el proceso de dividir las cadenas de texto de un documento en piezas más pequeñas que se denominan *tokens*.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Stemming:** </FONT>: Es la tarea en procesamiento de lenguaje natural de eliminar: sufijos, prefijos, etc. para obtener el tallo de la palabra:\n","\n","- Caminando : caminar\n","\n","- Anteriormente: anterior\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Lematización:** </FONT> es el proceso en el que se identifica y sustituye una palabra (plurales, femeninos, conjugaciones, etc) por su *lema* (raíz). Esto quiere decir por una palabra qe sea válida en el idioma.\n","\n","- Carros : Carro\n","\n","- Comiendo : comer\n","\n","- Entrenaría : entrenar\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Part of Speech (PoS)-Tagging:** </FONT> Proceso mediante el cual se asigna una etiqueta de categoría a las partes tokenizadas de una oración. Es decir, si es sustantivo, verbo, etc.\n","\n","    - Artículo o determinante: un, el, las\n","    - Sustantivo o nombre: hombre, tren, carro, mueble\n","    - Pronombre: yo, tu, el, ella  \n","    - Verbo: correr, pensar --acción\n","    - Adjetivo: alto, valente, nuevo.\n","    - Adverbio: tarde, temprano, anoche, constantemente.\n","    - Preposición: a, ante, con , contra, (unir palabras)\n","    - Conjunción: y, o, ni, que\n","    - Interjección: ¡ay! ,¡oh! (sentimiento)\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **n-grammas:** </FONT> modelo de representación para simplificar los contenidos de un texto. En los n-gramas nos interesa preservar secuencias contiguas de N-elementos de la selección de texto.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Stop Words:** </FONT> Son palabras que no aportan al significado de la oraciones : preprosiciones, conjunciones, adverbios, artículos, etc."],"metadata":{"id":"hC3PolDHfOJV"}},{"cell_type":"markdown","source":["Veamos algunos ejemplos de lo anterior. Para ello vamos a importar la librería *nltk*"],"metadata":{"id":"7TwggYwAgeG2"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"all\")"],"metadata":{"id":"IoUT4-sUgnUh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509631,"user_tz":300,"elapsed":54882,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"acacc101-a6ba-440b-f593-3cd95fc6715f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 1. tokenizar </FONT>"],"metadata":{"id":"Jb-ogq0mg1tT"}},{"cell_type":"code","source":["# separa cuando encuentra espacio\n","documento1 = \"Cien años! de sole.dad 100.56 es una novela del escritor colombiano\\\n","Gabriel García Márquez, ganador del Premio Nobel de Literatura en 1982.\\\n","Es considerada una obra maestra? de la literatura hispanoamericana y universal.\\\n","Así como una de las obras más traducidas y leídas en español.\"\n","words = nltk.word_tokenize(documento1)\n","print(words)"],"metadata":{"id":"FyP4NI22g3Zf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509631,"user_tz":300,"elapsed":7,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"7bf01446-b257-498c-b506-2cc310668c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Cien', 'años', '!', 'de', 'sole.dad', '100.56', 'es', 'una', 'novela', 'del', 'escritor', 'colombianoGabriel', 'García', 'Márquez', ',', 'ganador', 'del', 'Premio', 'Nobel', 'de', 'Literatura', 'en', '1982.Es', 'considerada', 'una', 'obra', 'maestra', '?', 'de', 'la', 'literatura', 'hispanoamericana', 'y', 'universal.Así', 'como', 'una', 'de', 'las', 'obras', 'más', 'traducidas', 'y', 'leídas', 'en', 'español', '.']\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 2. Stemming </FONT>\n","\n","- Funciona mejor para inglés"],"metadata":{"id":"_Iju0lxnhhG1"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","stm = PorterStemmer()"],"metadata":{"id":"zbtW-IwghqVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stm.stem('running'))\n","print(stm.stem('minimum'))\n","print(stm.stem('likely'))"],"metadata":{"id":"qBIMPiFJhsKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509632,"user_tz":300,"elapsed":4,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"66e2f971-78ed-4174-8ba6-5d828158cc38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["run\n","minimum\n","like\n"]}]},{"cell_type":"markdown","source":["Con la siguiente librería podemos usar otros idiomas. Pero dado que *nltk* está enfocado al inglés, funciona mejor con estos idiomas."],"metadata":{"id":"9UzQtKIhiSjn"}},{"cell_type":"code","source":["from nltk.stem.snowball import SnowballStemmer"],"metadata":{"id":"zneGsAsviFZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\" \".join(SnowballStemmer.languages))"],"metadata":{"id":"ut8AHFiFiDB6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509847,"user_tz":300,"elapsed":8,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"b0109509-b951-4617-c6a0-2bfc7954819d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"]}]},{"cell_type":"code","source":["stemmer = SnowballStemmer(\"spanish\")"],"metadata":{"id":"HQSBgn_qiI08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stemmer.stem(\"precisamente\"))\n","print(stemmer.stem(\"increible\"))"],"metadata":{"id":"LRe-zkv-iKhO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509848,"user_tz":300,"elapsed":8,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"69bbba02-58bc-483b-acb1-be24900157e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["precis\n","increibl\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 3. Stop Words </FONT>\n","\n"],"metadata":{"id":"Smm70vUuidoP"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","print(set(stopwords.words('spanish')))"],"metadata":{"id":"mjCC5SkNijmo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509848,"user_tz":300,"elapsed":6,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"f4d57863-edac-4ffe-db58-da30c5021512"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'tuvo', 'vuestros', 'tenido', 'tendremos', 'tendrán', 'sea', 'fuisteis', 'éramos', 'vuestra', 'otro', 'estamos', 'esté', 'entre', 'me', 'hubisteis', 'con', 'tuvieron', 'estada', 'ha', 'por', 'un', 'eso', 'son', 'hubieran', 'soy', 'estarías', 'mías', 'donde', 'algunos', 'estemos', 'todo', 'esta', 'vuestro', 'o', 'habrán', 'hubiste', 'serían', 'fueron', 'hubiesen', 'estuviese', 'tuviésemos', 'han', 'y', 'sintiendo', 'suya', 'estéis', 'fuese', 'fuerais', 'habiendo', 'nosotras', 'habían', 'estos', 'nuestras', 'tuvieseis', 'tienes', 'estuvo', 'están', 'algunas', 'tuvimos', 'todos', 'ellas', 'sois', 'le', 'estuviésemos', 'tengáis', 'lo', 'hay', 'haya', 'hemos', 'nada', 'estuviesen', 'estas', 'estábamos', 'estuvisteis', 'otras', 'habréis', 'tendréis', 'sus', 'otra', 'estuvieses', 'vosotros', 'estaría', 'estarían', 'serías', 'tuviesen', 'muchos', 'el', 'habrás', 'estuvieran', 'les', 'estad', 'seamos', 'tuyo', 'estaban', 'habías', 'fuera', 'nosotros', 'habéis', 'fueran', 'estuvieseis', 'fuesen', 'al', 'suyas', 'tenidos', 'tengo', 'hubiésemos', 'tendrías', 'sí', 'que', 'fueras', 'has', 'esto', 'habremos', 'porque', 'estabas', 'tendrá', 'está', 'tendríamos', 'tenéis', 'uno', 'cuando', 'quienes', 'ella', 'tuviste', 'él', 'habré', 'sobre', 'habíamos', 'tuvieses', 'tienen', 'fui', 'habidas', 'antes', 'fueseis', 'estaríamos', 'tanto', 'muy', 'sentido', 'estuvieras', 'nuestro', 'estarán', 'tuviese', 'estaré', 'estuvimos', 'tenga', 'ese', 'estado', 'para', 'tendría', 'ni', 'tiene', 'estuviéramos', 'habrías', 'mía', 'hubiéramos', 'estuvierais', 'hubieras', 'fuésemos', 'nos', 'estuviste', 'su', 'tuve', 'a', 'tened', 'estuviera', 'hasta', 'ante', 'estaréis', 'se', 'de', 'tenemos', 'estoy', 'sin', 'contra', 'algo', 'hube', 'tenías', 'teníais', 'estuve', 'poco', 'serás', 'sentid', 'tenía', 'habíais', 'tus', 'he', 'yo', 'te', 'estuvieron', 'fuiste', 'teníamos', 'los', 'siente', 'tengas', 'esas', 'tu', 'qué', 'tengan', 'hayas', 'sean', 'hubieron', 'sentidas', 'tuviera', 'unos', 'tuvieran', 'nuestros', 'estarás', 'fuimos', 'suyo', 'míos', 'habido', 'tendrás', 'sería', 'ti', 'habrían', 'hubierais', 'habidos', 'tú', 'durante', 'seáis', 'seríamos', 'habría', 'habrá', 'mío', 'hubiese', 'tenían', 'seas', 'quien', 'estará', 'nuestra', 'cual', 'las', 'tuviéramos', 'fuéramos', 'tendrían', 'tendríais', 'también', 'ellos', 'desde', 'pero', 'e', 'estás', 'del', 'tuvierais', 'estabais', 'teniendo', 'vosotras', 'hubimos', 'había', 'estén', 'erais', 'sentidos', 'habríamos', 'esa', 'como', 'seréis', 'tendré', 'la', 'vuestras', 'en', 'eras', 'estados', 'tuvieras', 'habida', 'eres', 'estaremos', 'hubo', 'tuya', 'seremos', 'estés', 'serán', 'mis', 'seré', 'os', 'no', 'ya', 'mucho', 'esos', 'era', 'estaba', 'hayan', 'tengamos', 'estar', 'hayamos', 'hubieseis', 'otros', 'hubiera', 'una', 'eran', 'mi', 'estáis', 'tuyos', 'estando', 'tuvisteis', 'hayáis', 'mí', 'es', 'seríais', 'habríais', 'estadas', 'tenida', 'hubieses', 'este', 'tenidas', 'tuyas', 'suyos', 'más', 'fue', 'será', 'estaríais', 'sentida', 'somos', 'fueses'}\n"]}]},{"cell_type":"code","source":["doc = \"Un radar multa a Fabian Sanchez por conducir demasiado rapido en la autopista\"\n","words = nltk.word_tokenize(doc)\n","for word in words:\n","        if word in stopwords.words('spanish'):\n","            print (word)"],"metadata":{"id":"dGTsiZL9imHS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377509848,"user_tz":300,"elapsed":5,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"2dd8bfdb-814e-4ef8-c536-81cbb58b0893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a\n","por\n","en\n","la\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 4. Lematización </FONT>"],"metadata":{"id":"u6-l6vAHio1i"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","lemm = WordNetLemmatizer()"],"metadata":{"id":"czyOecb1iub-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lemm.lemmatize('dogs'))\n","print(lemm.lemmatize('houses'))\n","print(lemm.lemmatize('perros'))\n"],"metadata":{"id":"0somr_U0iwA6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511744,"user_tz":300,"elapsed":1900,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"a25649e9-efea-420b-8fa9-e9c6056e8587"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dog\n","house\n","perros\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 5. n-grams </FONT>\n","\n","Los **n-gram** son un modelo de representación que selecciona secuencias contiguas de *n*-elementos del texto\n","\n","- Veamos un ejemplo"],"metadata":{"id":"osOlGHB_3q5y"}},{"cell_type":"code","source":["from nltk import ngrams\n"],"metadata":{"id":"KZhKl1fz4AiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc = \"Un radar multa a Fabian Sanchez por conducir demasiado rapido en la autopista\"\n","words = nltk.word_tokenize(doc)\n","num_elementos = 3\n","n_grams = ngrams(words, num_elementos)\n","for grams in n_grams:\n","    print (grams)"],"metadata":{"id":"Q4GNZSEtjKYd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511745,"user_tz":300,"elapsed":4,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"b5d30cb4-9307-4848-e2cc-59823aa7e33f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('Un', 'radar', 'multa')\n","('radar', 'multa', 'a')\n","('multa', 'a', 'Fabian')\n","('a', 'Fabian', 'Sanchez')\n","('Fabian', 'Sanchez', 'por')\n","('Sanchez', 'por', 'conducir')\n","('por', 'conducir', 'demasiado')\n","('conducir', 'demasiado', 'rapido')\n","('demasiado', 'rapido', 'en')\n","('rapido', 'en', 'la')\n","('en', 'la', 'autopista')\n"]}]},{"cell_type":"markdown","source":["<FONT SIZE=4 COLOR=\"purple\"> 6. Etiquetado Formológico- PoS (part of speech-tagging) </FONT>\n","\n","NLTK nos da algunas herramientas para crear etiquetadores morfológicos, es decir, identificar si una palabra es verbo, sustantivo, adjetivo , etc.\n","\n","CC : coodinate conjunction\n","\n","CD : cardinal number\n","\n","DT : determiner\n","\n","EX ; existencial there\n","\n","FW : foeign word\n","\n","IN : preprosition or subordinating\n","\n","JJ : adjetive\n","\n","JJR : adjetive, comparative\n","\n","JJS : adjetive superlative\n","...\n","BV : verbo forma base\n","\n","VBD : verbo pasado\n","\n","VBG : verb gerundio\n","\n","Para mayor información ver\n","\n","[TAGGING](https://pythonspot.com/nltk-speech-tagging/)\n","\n","Usaremos **nltk.pos_tag** que es un etiquetador morfológico basado en aprendizaje automático. A partir de miles de ejemplos de oraciones etiquetadas manualmente, el sistema *ha aprendido*, calculando frecuencias y generalizando cuál es la categoría gramatical más probable para cada token."],"metadata":{"id":"xDq2n4pQjtZN"}},{"cell_type":"code","source":["oracion1 = \"This is the lost dog I found at the park\".split()\n","oracion2 = \"The progress of the humankind as I progress\".split()\n","oracion3 = \"When I went to the University, I played soccer\".split()\n","print(nltk.pos_tag(oracion1))\n","print(nltk.pos_tag(oracion2))\n","print(nltk.pos_tag(oracion3))"],"metadata":{"id":"1MqZiYnLkFEI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511957,"user_tz":300,"elapsed":214,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"b023b9cd-32bc-4b87-97eb-be200ae27794"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('lost', 'JJ'), ('dog', 'NN'), ('I', 'PRP'), ('found', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('park', 'NN')]\n","[('The', 'DT'), ('progress', 'NN'), ('of', 'IN'), ('the', 'DT'), ('humankind', 'NN'), ('as', 'IN'), ('I', 'PRP'), ('progress', 'VBP')]\n","[('When', 'WRB'), ('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('University,', 'NNP'), ('I', 'PRP'), ('played', 'VBD'), ('soccer', 'NN')]\n"]}]},{"cell_type":"markdown","source":["<FONT SIZE=4 COLOR=\"red\"> Observación: </FONT>\n","\n","### Etiquetador basado en Expresiones Regulares\n","\n","- Las expresiones regulares nos permiten especificar cadenas de texto.\n","\n","- Esta nos permite identificar patrones en las palabras para poderlas clasificar\n","\n","- Vamos a crear nuestra propia lista para etiquetar. (tagging)\n","\n","- El primer elemento es la expresión regular y la segunda la categoría gramatical.\n","\n","- Usamos la instrucción **RegexpTagger**.\n","\n","- Este ejemplo lo haremos en inglés\n","\n","Usaremos Raw String Notation *(r\"texto\")* : una forma en la cual Python nos permite implementar expresiones regulares.\n","\n"],"metadata":{"id":"JfH19KRUkOIH"}},{"cell_type":"markdown","source":["Veamos un par de ejemplos"],"metadata":{"id":"1KSy7wRumOy5"}},{"cell_type":"code","source":["pattern = [(r\"He\", \"TAG\")]\n","tagger=nltk.RegexpTagger(pattern)\n","\n","print(tagger.tag(\"He was born He in March 1991\".split()))"],"metadata":{"id":"K7sYEv9fkg87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511958,"user_tz":300,"elapsed":9,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"a7109eb6-88af-4ce3-954f-70842840016a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('He', 'TAG'), ('was', None), ('born', None), ('He', 'TAG'), ('in', None), ('March', None), ('1991', None)]\n"]}]},{"cell_type":"code","source":["patrones = [(r\"[Aa]m$\", \"Verb-to-be\"),\n","            (r\".*ly$\", \"adverb\"),\n","            (r\"[Ww]ere\", \"Verb-to-be-past\"),\n","            (r\"[Tt]hey$\", \"pron\"),\n","            (r\".*ing$\", \"gerun\"),\n","            (r'.*ed$', 'simple past')]\n","tagger=nltk.RegexpTagger(patrones)\n","print(tagger.tag(\"They were playing soccer fastly . I am happy\".split()))"],"metadata":{"id":"pV4qWQm3mWqK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511958,"user_tz":300,"elapsed":7,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"53a98a83-0b2c-4ded-fd3e-a57dd842e12a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('They', 'pron'), ('were', 'Verb-to-be-past'), ('playing', 'gerun'), ('soccer', None), ('fastly', 'adverb'), ('.', None), ('I', None), ('am', 'Verb-to-be'), ('happy', None)]\n"]}]},{"cell_type":"code","source":["print(nltk.pos_tag(\"They were playing soccer fastly . I am happy\".split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EF8fNk_nRXpx","executionInfo":{"status":"ok","timestamp":1692377511958,"user_tz":300,"elapsed":6,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"9dc5d399-27b4-476d-c89c-3ff0ac2730b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('They', 'PRP'), ('were', 'VBD'), ('playing', 'VBG'), ('soccer', 'NN'), ('fastly', 'RB'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('happy', 'JJ')]\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=5 COLOR=\"purple\"> Bag of Words </FONT>\n","\n","- *Bag of Words* o ***Bolsa de Palabras*** (***BoW***) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto (o corpus).\n","\n"],"metadata":{"id":"5QdN1EcOVFpZ"}},{"cell_type":"code","source":["documento = [\"carlos juega mucho carlos carlos carro carro juega universidad universidad juega juega  mucho mucho carlos\",\n","             \"conduce conduce conduce carro carro mucho mucho carro carro carro carro carro\",\n","             \"universidad universidad MACC MACC MACC MACC MACC MACC UR UR UR UR UR UR UR\",\n","             \"universidad universidad universidad carlos carlos conduce conduce carlos carlos carlos UR UR UR UR\",\n","             \"universidad universidad carlos carlos carlos conduce conduce carro carro carro carro mucho mucho mucho\",\n","             \"carlos juega universidad carro mucho juega mucho\"]"],"metadata":{"id":"QL2mjkNoVNIm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A continuación creamos una bolsa de palabras con las frecuencias."],"metadata":{"id":"ovXnk5-wrjrG"}},{"cell_type":"code","source":["# Construimos la bolsa de palabras\n","bow = dict()\n","for doc in documento:\n","    doc = doc.split(\" \")\n","    for palabra in doc:\n","        if palabra in bow:\n","            bow[palabra] += 1\n","        else:\n","            bow[palabra] = 1\n","print(bow)"],"metadata":{"id":"AxgKnaPwVOBm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511958,"user_tz":300,"elapsed":5,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"d5fb9f6c-0c8c-43a1-e80e-4da6beade525"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 13, 'juega': 6, 'mucho': 10, 'carro': 14, 'universidad': 10, '': 1, 'conduce': 7, 'MACC': 6, 'UR': 11}\n"]}]},{"cell_type":"markdown","source":["**Ejercicio en clase:**\n","\n","Utilice la librería ***nltk*** para generar en una línea de código el resultado anterior."],"metadata":{"id":"dy8QO178rxp8"}},{"cell_type":"code","source":[],"metadata":{"id":"EA0Oad7zr8NC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Existen más formas de construir bolsas de palabras. Las más utilizadas son las siguiente:\n","\n","  1. Vectores de Frecuencias (TF)\n","  2. One-Hot-Encode\n","  3. Term Frequency-Inverse Document Frequency (TF-IDF)\n","    \n","Vamos a abordar cada una de las anteriores formas y utilizaremos las siguientes librerías para su implementación.\n","\n","  * ***scikit***\n","  * ***NLTK***\n","  * ***Gensim***\n","\n","¿Por qué es necesario conocer las tres?\n","\n","La respuesta a esta pregunta radica en que hay librerías especializadas para determinados modelos, y en general, debemos hacer una bolsa de palabras antes de aplicar algunos modelos. Por ejemplo, *Gensim* es muy usada para toping-modeling y LDA, etc.\n","    \n","\n","\n","\n","### -Scikit\n","\n","* Esta librería devuelve una matriz en la que las **filas representan a cada documento del corpus** y las **columnas el número de apariciones de las palabras**.\n","\n","\n","* Para saber que palabra corresponde a cada columan de la matriz, scikit nos devuelve una lista en la que coinciden los indice de cada una de las palabras de la lista con la matriz.\n","\n","\n","* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"],"metadata":{"id":"sG5QEPy3VVAz"}},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 1. Vectores de Frecuencias </FONT>\n","\n","- Los ***vectores de frecuencias*** es el método más sencillo de construir las ***Bolsas de Palabras***.\n","\n","- Simplemente consiste en contar cuantas veces aparece una palabra en el documento del corpus."],"metadata":{"id":"BUyQz6q0svag"}},{"cell_type":"code","source":["# para hacer TF en sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","vectors = vectorizer.fit_transform(documento)\n","\n","# Resultados\n","print(vectorizer.get_feature_names_out())\n","print(vectors.toarray())"],"metadata":{"id":"gRzadR4YVXJB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377511959,"user_tz":300,"elapsed":4,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"78cfaaf0-fa4b-4915-c7fc-6f4f05e6c65c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['carlos' 'carro' 'conduce' 'juega' 'macc' 'mucho' 'universidad' 'ur']\n","[[4 2 0 4 0 3 2 0]\n"," [0 7 3 0 0 2 0 0]\n"," [0 0 0 0 6 0 2 7]\n"," [5 0 2 0 0 0 3 4]\n"," [3 4 2 0 0 3 2 0]\n"," [1 1 0 2 0 2 1 0]]\n"]}]},{"cell_type":"markdown","source":["### Gensim\n","\n","* ***Gensim trabaja con \"Diccionarios\"*** (gensim.corpora.Dictionary) que es una estructura de datos en la que guarda el orden de las palabras que hay en el corpus.\n","\n","\n","* Posteriormente se construye la Bolsa de Palabras con las frecuencias con la función \"***doc2bow***\"\n","\n","\n","* Como resultado devuelve una lista por cada documento en la que se indicida por cada palabra del documento identificada por el 'id' del diccionario previamente construido la frecuencia de esa palabra en el documento.\n","\n","\n","* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/corpora/dictionary.html"],"metadata":{"id":"rQ5nQYhjVewC"}},{"cell_type":"code","source":["import gensim\n","# primero tokenizamos\n","tokenize = [nltk.word_tokenize(text) for text in documento]\n","# creamos un diccionario\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","# utilizamos doc2bow\n","vectors = [dictionary.doc2bow(token) for token in tokenize]\n","\n","# codigos de las palabras\n","print(dictionary.token2id)\n","# frecuencias\n","print('\\nApariciones de las palabras en los documentos:')\n","vectors"],"metadata":{"id":"D7DHzy2_VguA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377512796,"user_tz":300,"elapsed":840,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"7af02c64-350a-4192-802c-4a2feb278050"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 0, 'carro': 1, 'juega': 2, 'mucho': 3, 'universidad': 4, 'conduce': 5, 'MACC': 6, 'UR': 7}\n","\n","Apariciones de las palabras en los documentos:\n"]},{"output_type":"execute_result","data":{"text/plain":["[[(0, 4), (1, 2), (2, 4), (3, 3), (4, 2)],\n"," [(1, 7), (3, 2), (5, 3)],\n"," [(4, 2), (6, 6), (7, 7)],\n"," [(0, 5), (4, 3), (5, 2), (7, 4)],\n"," [(0, 3), (1, 4), (3, 3), (4, 2), (5, 2)],\n"," [(0, 1), (1, 1), (2, 2), (3, 2), (4, 1)]]"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 2. (TF) One-Hot-Encode </FONT>\n","\n","- Este método de construcción de la Bolsa de Palabras consiste en indicar con un flag ([0,1], [True, False], etc.) si una palabra aparece o no en el documento.\n","\n","Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer"],"metadata":{"id":"2y0NmXJ4Vi_A"}},{"cell_type":"code","source":["# para genetar one-hot.encode con sklearn\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import Binarizer\n","\n","vectorizer = CountVectorizer()\n","corpus = vectorizer.fit_transform(documento)\n","\n","onehot = Binarizer()\n","corpus = onehot.fit_transform(corpus.toarray())\n","\n","# Resultados\n","print(vectorizer.get_feature_names_out())\n","print(corpus)"],"metadata":{"id":"YdfIk5xKVkM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377512796,"user_tz":300,"elapsed":9,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"f44650b7-f8a9-4ff4-99a1-b7f47a5ae99c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['carlos' 'carro' 'conduce' 'juega' 'macc' 'mucho' 'universidad' 'ur']\n","[[1 1 0 1 0 1 1 0]\n"," [0 1 1 0 0 1 0 0]\n"," [0 0 0 0 1 0 1 1]\n"," [1 0 1 0 0 0 1 1]\n"," [1 1 1 0 0 1 1 0]\n"," [1 1 0 1 0 1 1 0]]\n"]}]},{"cell_type":"code","source":["# Para hacerlo en nltk\n","import nltk\n","\n","def tokenize(text):\n","    for token in nltk.word_tokenize(text):\n","        yield token\n","\n","def vectorize(corpus):\n","    return {token: True for token in tokenize(corpus)}\n","\n","vectors = map(vectorize, documento)\n","\n","# Resultados\n","for v in vectors:\n","    print(v)"],"metadata":{"id":"NoQFSzGoVns2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377512797,"user_tz":300,"elapsed":8,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"50a89a10-2f6c-4e69-f720-fb3ed2a6edbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': True, 'juega': True, 'mucho': True, 'carro': True, 'universidad': True}\n","{'conduce': True, 'carro': True, 'mucho': True}\n","{'universidad': True, 'MACC': True, 'UR': True}\n","{'universidad': True, 'carlos': True, 'conduce': True, 'UR': True}\n","{'universidad': True, 'carlos': True, 'conduce': True, 'carro': True, 'mucho': True}\n","{'carlos': True, 'juega': True, 'universidad': True, 'carro': True, 'mucho': True}\n"]}]},{"cell_type":"markdown","source":["**Nota:** En Gensim, la estructura de salida es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos solo tendrán valor $1$.\n","\n","Para más información ver el siguiente enlace:\n","\n","https://radimrehurek.com/gensim/corpora/dictionary.html"],"metadata":{"id":"Bwvl9cOiVqjM"}},{"cell_type":"code","source":["import gensim\n","\n","tokenize = [nltk.word_tokenize(text) for text in documento]\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in tokenize]\n","\n","# codificación\n","print(dictionary.token2id)\n","# bolsa de palabras\n","print('\\nApariciones de las palabras en los documentos (id, 1):')\n","vectors"],"metadata":{"id":"6UK9T9KoVrzU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377512797,"user_tz":300,"elapsed":7,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"bd453612-c172-4b34-9814-ec78a3a66e92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 0, 'carro': 1, 'juega': 2, 'mucho': 3, 'universidad': 4, 'conduce': 5, 'MACC': 6, 'UR': 7}\n","\n","Apariciones de las palabras en los documentos (id, 1):\n"]},{"output_type":"execute_result","data":{"text/plain":["[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n"," [(1, 1), (3, 1), (5, 1)],\n"," [(4, 1), (6, 1), (7, 1)],\n"," [(0, 1), (4, 1), (5, 1), (7, 1)],\n"," [(0, 1), (1, 1), (3, 1), (4, 1), (5, 1)],\n"," [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 3. Term Frequency-Inverse Document Frequency (TF-IDF) </FONT>\n","\n","- El TF-IDF (Frecuencia de Termino - Frecuencia Inversa de Documento) es una medida numérica que permite expresar como de relevante es una palabra para un documento en una colección de documentos (o corpus).\n","\n","- La fórmula que se aplica es\n","\n","$$idf(t_i) = \\log \\left (\\dfrac{N}{n_i} \\right)$$\n","\n","donde\n","\n","  - $t_i$ es el término $i$.\n","  - $n_i$ es número de documentos donde aparace el termino $i$.\n","  - $N$ número de documentos.\n","\n","\n","## <FONT SIZE=3 COLOR=\"blue\"> Motivación de TF+iDF </FONT>\n","\n","- **Referencia de consulta:** Understanding Inverse Document Frequency:\n","On theoretical arguments for IDF. Stephen Robertson. Microsoft Research.\n","\n","- En el campo de la información, la **entropía** (Shannon) mide la incertidumbre de una fuente de información.\n","\n","- También se puede decir, que es la información promedio que contienen los símbolos usados.\n","\n","- En ese orden de ideas, las palabras que son más frecuentes aportan menos información. (el, a de, que). Si uno las quita de un documento, podría entender más de este que si quita una palabra que se repita menos.\n","\n","- La ley de **Zipf**: es una regla empírica que indica que la frecuencia de aparición de distintas palabras sigue una distribución que es aproximadamente\n","\n","$$P_n \\approx \\dfrac{1}{a^n}$$\n","\n","donde, $P_n$ es la frecuencia de la n-ésima palabra más frecuente y $a>1$.\n","\n","- La entropía termodinámica es igual a la entropía de la teoría de la información de esa distribución (medida usando el logaritmo neperiano) multiplicada por la constante de Boltzmann k. $k\\log N$.\n","\n","- Supongamos que tenemos un evento con grado de indeterminación $k$ o en otras palabras, que existen k estados posibles y todos son equiprobables. Luego\n","\n","$$probabilidad(ocurrencia) = \\dfrac{1}{k}$$.\n","\n","$$P(t_i)= \\dfrac{n_i}{N}$$\n","$$idf(t_i) = - \\log (P(t_i)) = \\log (\\dfrac{N}{n_i})$$\n","\n","Veamos las dos definiciones de manera precisa:\n","\n","***Frecuencia del Término (Frequency Term-TF):***\n","\n","- Es la frecuencia de una palabra en cada documento del corpus.\n","\n","- Relación entre el número de veces que aparace la palabra en el documento comparada con el total de palabras de ese documento.\n","\n","$$TF(i,j) = tf_{ij}= \\dfrac{n_{i,j}}{\\sum_\\limits{j} n_{i,j}}  $$\n","\n","$$tf_{ij}=\\dfrac{\\text{# de apariciones de la palabra j}}{\\text{total de palabras en el documento i}}$$\n","\n","donde:\n","\n","- $tf_{i,j}$: es la frecuencia (relativa) de la palabra $j$ en el documento $i$\n","\n","- $n_{i,j}$: es el número de veces que aparece la palabra $j$ en el documento $i$.\n","\n","***Frecuencia inversa de documento (Inverse Document Frequency iDF):***\n","\n","- Es una medida que permite calcular el peso de las palabras en los documentos del corpus.\n","\n","- Si una palabra no aparece en mucho documentos su $iDF$ debe ser alto, comparada con una que aparezca en más documentos.\n","\n","$$idf(t_j) = \\log \\left (\\dfrac{N}{n_j} \\right)$$\n","\n","donde\n","\n","  - $t_j$ es una palabra del corpus\n","  - $n_j$ es número de documentos donde aparace el término o palabra $j$.\n","  - $N$ número de documentos.\n","\n","\n"," ***Fórmula TF+iDF***\n","\n"," $$TF+iDF(t_j) = tf(t_j)*idf(t_j)$$\n","\n","$$ tf(t_j)*idf(t_j) =\\dfrac{\\text{# de apariciones de la palabra $t_j$ en el documento i}}{\\text{total de palabras en el documento i}} * \\dfrac{\\text{ total de documentos en el corpus}}{\\text{# de documentos donde aparace la palabra $t_j$}}  $$\n","\n","\n","**Observaciones**\n","\n","- Construir la Bolsa de Palabras con TF-IDF en vez de con frecuencias evita dar \"importancia\" a texto muy largos y con mucha repetición de palabras, frente a textos cortos y con pocas repeticiones de palabras.\n","\n","- En la literatura también podemos encontrarla en escala logarítmica así: También se puede definir de la siguiente manera\n","\n","* ***TF*** (Term Frecuency): Es la frecuencia con la que aparece la palabra en un documento del corpus. Esta se define como:\n","    \n","    $$tf(t,d) = 1 + log(f_{t,d})$$\n","    \n","* ***IDF*** (Inverse Document Frequency): La frecuencia inversa del documento nos indica lo común que es una palabra en el corpus.\n","    \n","    $$idf(t,D) = log(1 + \\frac{N}{n_t})$$\n","    \n","* ***TF-IDF*** queda definido como:\n","$$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)$$\n","\n","**Implementación:**\n","\n","* Las implementaciones del ***TF-IDF*** de **Scikit** y **Gensim** estan pensadas para corpus con un número relevante de documentos y de palabras, por tanto la implementación del ***TF-IDF*** acepta una serie de parámetros para no tener en cuenta Stop Words, palabras irrelevantes, etc. por lo que si se realiza una implementación del ***TF-IDF*** según la bibliografia no van a conincidir los resultados de esa implementación con los resultados de las librerías de **Scikit** y **Gensim** a no ser que se modifiquen los parámetros de las funciones del ***TF-IDF***.\n","\n","\n","Scikit\n","\n","* Las estructuras de datos de salida de ***Scikit*** son iguales de en el caso de la construcción del vector de frecuencias salvo que en el contenido de la matriz será de números decimales en vez de números enteros.\n","\n","\n","* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n"],"metadata":{"id":"bmRkFNtOVvlq"}},{"cell_type":"markdown","source":["- Veamos un ejemplo dado el siguiente corpus de 3 documentos con las siguientes palabras:\n","\n","```\n","corpus = [\"messi messi jordan\",\n","          \"jordan messi carro jordan carro\",\n","          \"avión messi mesa\"]\n","```\n","$$$$\n","<center>\n","<table class=\"default\">\n","<tr>\n","    <th scope=\"row\">TF</th>\n","    <th>messi</th>\n","    <th>jordan</th>\n","    <th>carro</th>\n","    <th>avión</th>\n","    <th>mesa</th>\n","</tr>\n","<tr>\n","    <th>$d_1$</th>\n","    <th>$\\dfrac{2}{3}$</th>\n","    <th>$\\dfrac{1}{3}$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","</tr>\n","<tr>\n","    <th>$d_2$</th>\n","    <th>$\\dfrac{1}{5}$</th>\n","    <th>$\\dfrac{2}{5}$</th>\n","    <th>$\\dfrac{2}{5}$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","</tr>\n","<tr>\n","    <th>$d_3$</th>\n","    <th>$\\dfrac{1}{3}$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","    <th>$\\dfrac{1}{3}$</th>\n","    <th>$\\dfrac{1}{3}$</th>\n","</tr>\n","</table>\n","</center>\n","\n","- Por ejemplo. La frecuencua de *messi* en el documento 1 es\n","\n","$$TF_{11}=d_1(messi) = \\dfrac{2}{3}$$\n","\n","$$TF_{24}=d_2(avión) = 0$$\n","\n","Ahora, veamos $idf$\n","\n","$$$$\n","<center>\n","<table class=\"default\">\n","<tr>\n","    <th scope=\"row\">TF+iDF</th>\n","    <th>--messi--   </th>\n","    <th>--jordan--  </th>\n","    <th>--carro--   </th>\n","    <th>--avión--   </th>\n","    <th>--mesa--    </th>\n","</tr>\n","\n","<tr>\n","    <th>$idf$</th>\n","    <th>$log \\left(\\dfrac{3}{3}\\right)$</th>\n","    <th>$log \\left(\\dfrac{3}{2}\\right)$</th>\n","    <th>$log \\left(\\dfrac{3}{1}\\right)$</th>\n","    <th>$log \\left(\\dfrac{3}{1}\\right)$</th>\n","    <th>$log \\left(\\dfrac{3}{1}\\right)$</th>\n","</tr>\n","\n","\n","\n","<tr>\n","    <th>$d_1$</th>\n","    <th>$0$</th>\n","    <th>$0.06$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","</tr>\n","<tr>\n","    <th>$d_2$</th>\n","    <th>$0$</th>\n","    <th>$0.07$</th>\n","    <th>$0.19$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","</tr>\n","<tr>\n","    <th>$d_3$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","    <th>$0$</th>\n","    <th>$0.16$</th>\n","    <th>$0.16$</th>\n","</tr>\n","</table>\n","</center>\n","\n","Por ejemplo\n","\n","$$df-idf(2,3) = tf(2,3)*idf(2,3)=\\dfrac{2}{5}*\\log(3)=0.19$$\n","\n","*Análisis de la tabla:*\n","\n","- La palabra *messi* tiene valor 0 ya que está en los tres documentos, esto quiere decir que no aporta información relevante para discriminar los documentos o identificar a los mismos.\n","\n","- Las palabras *avión* , *carro* y *mesa* tienen valores altos ya que sus frecuencias de aparación entre documentos es baja.\n","\n"],"metadata":{"id":"u9SbDcAu4Du5"}},{"cell_type":"code","source":["corpus = [\"messi messi jordan\",\n","          \"jordan messi carro jordan carro\",\n","          \"avión messi mesa\"]"],"metadata":{"id":"eeSTLvT474jp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a calcular **TF+iDF** con las librerías que hemos venido trabajando"],"metadata":{"id":"kEKYewOdMm0v"}},{"cell_type":"code","source":[],"metadata":{"id":"7Dwp4rSQBHV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### -Gensim\n","\n","* La estructura de datos de salida de ***Gensim*** es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos tendrán números decimales en vez de números enteros.\n","\n","\n","* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/models/tfidfmodel.html"],"metadata":{"id":"khvE4x8tVyUn"}},{"cell_type":"code","source":["import gensim\n","\n","tokenize = [nltk.word_tokenize(text) for text in corpus]\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","tfidf = gensim.models.TfidfModel(dictionary=dictionary, normalize=True)\n","vectors = [tfidf[dictionary.doc2bow(doc)] for doc in tokenize]\n","\n","# Resultados\n","print('Diccionario de palabras -> palabra: id\\n')\n","print(dictionary.token2id)\n","print('\\nApariciones de las palabras en los documentos (id, tfidf):')\n","vectors"],"metadata":{"id":"Xkmku2vwV041","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377512797,"user_tz":300,"elapsed":5,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"6b1fc05a-ce12-4452-ae48-aece6b03fbd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Diccionario de palabras -> palabra: id\n","\n","{'jordan': 0, 'messi': 1, 'carro': 2, 'avión': 3, 'mesa': 4}\n","\n","Apariciones de las palabras en los documentos (id, tfidf):\n"]},{"output_type":"execute_result","data":{"text/plain":["[[(0, 1.0)],\n"," [(0, 0.34624155305796134), (2, 0.9381453975456102)],\n"," [(3, 0.7071067811865475), (4, 0.7071067811865475)]]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Observe que lo anterior indica que:\n","\n","- Apesar de que *messi* tiene una mayor frecuencia en el primer documento no es la más relevante o característica del texto.\n","\n","- En el documento 2, la palabra más relevante es *carro*.\n","\n","- En el documento 3 hay dos palabras con igual relevancia o claves del texto."],"metadata":{"id":"Q7DfspoQSxdg"}},{"cell_type":"markdown","source":["A continuación, otras formas de cálcular *TF+iDF*"],"metadata":{"id":"58_2HLRdT_c-"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(min_df=1)\n","\n","# Pasamos los tweets normalizados a Bolsa de palabras\n","X = tfidf.fit_transform(corpus)\n","X\n","# Resultados\n","print(tfidf.get_feature_names_out ())\n","print(X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVIK3IhUSKqI","executionInfo":{"status":"ok","timestamp":1692377513103,"user_tz":300,"elapsed":310,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"931b7c19-bf6d-464c-c2df-ed9e571e7ba1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['avión' 'carro' 'jordan' 'mesa' 'messi']\n","[[0.         0.         0.54134281 0.         0.84080197]\n"," [0.         0.77484319 0.58928822 0.         0.22881744]\n"," [0.65249088 0.         0.         0.65249088 0.38537163]]\n"]}]},{"cell_type":"code","source":["feature_names = tfidf.get_feature_names_out()\n","for col in X.nonzero()[1]:\n","    print (feature_names[col], ' - ', X[0, col])"],"metadata":{"id":"-g7wuPa5SNJy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692377513104,"user_tz":300,"elapsed":2,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"7bf8aa74-8dbc-4a3d-90b5-136603c6b4c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["jordan  -  0.5413428136679054\n","messi  -  0.8408019731721111\n","carro  -  0.0\n","jordan  -  0.5413428136679054\n","messi  -  0.8408019731721111\n","mesa  -  0.0\n","avión  -  0.0\n","messi  -  0.8408019731721111\n"]}]},{"cell_type":"markdown","source":["Finalmente, un ejemplo con la variable documento definida inicialmente."],"metadata":{"id":"zY5wXmpsUG4-"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"u7aBwORrgKN4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <FONT SIZE=3 COLOR=\"blue\"> Ejercicio en clase </FONT>\n","\n","Considere el corpus que se da a continuación:\n","\n","1. Normalice el corpus.\n","  \n","   - convierta a minúscula ( Fabian ---> fabian)\n","   - reemplace las tildes ( acción---> accion)\n","   - Quitar signos de puntuación y caracteres especiales (Hola!---hola)\n","   - Elimine las stop-words.\n","\n","2. Aplique la función ***TfidfVectorizer(min_df=1)***\n","\n","3. Seleccione las cinco palabras claves de cada documento.\n","\n","   "],"metadata":{"id":"DcUSCCrtocRD"}},{"cell_type":"code","source":["import re\n","import string\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from urllib.request import urlopen"],"metadata":{"id":"9Qq7zaJ9oT2o","executionInfo":{"status":"ok","timestamp":1692378941281,"user_tz":300,"elapsed":527,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["corpus=[\"Las matemáticas son una de las ciencias más antiguas. Floreció primero antes de la antigüedad en Mesopotamia, 19​ En cuanto a la geometría20​, India y China, y más tarde en la antigüedad en Grecia y el helenismo. De ahí data la orientación hacia la tarea de demostración puramente lógica y la primera axiomatización, a saber, la geometría euclidiana20​. En la Edad Media sobrevivió de forma independiente en el primer humanismo de las universidades y en el mundo árabe.\",\n","           \"La guerra es la forma de conflicto sociopolítico más grave entre dos o más grupos humanos. Se da tanto en sociedades tribales como en civilizadas, pero es más grave entre estas últimas ya que son más complejas, masificadas y tecnificadas. Es quizás la más antigua de las relaciones internacionales y ya en el comienzo de las civilizaciones se constata el enfrentamiento organizado de grupos humanos armados con el propósito de controlar recursos naturales o humanos (conflictos entre cazadores nómadas y recolectores sedentarios que sí desarrollaron el concepto de propiedad),3​ exigir un desarme o imponer algún tipo de tributo, ideología, nacionalidad o religión, sometiendo, despojando y, en su caso, destruyendo al enemigo. Es más, este tipo de conducta gregaria es extensible a la mayor parte de los homínidos4​ y se encuentra estrechamente relacionado con el concepto etológico de territorialidad.\",\n","           \"La biología se ocupa tanto de la descripción de las características y los comportamientos de los organismos individuales, como de las especies en su conjunto, así como de las relaciones entre los seres vivos y de las interacciones entre ellos y el entorno. De este modo, trata de estudiar la estructura y la dinámica funcional comunes a todos los seres vivos, con el fin de establecer las leyes generales que rigen la vida orgánica y los principios de ésta.6​\",\n","           \"Las ciencias de la computación o ciencias de la informática son las ciencias formales que abarcan las bases teóricas de la información y la computación, así como su aplicación en los sistemas informáticos.1​2​3​ El cuerpo de conocimiento de las ciencias de la computación es frecuentemente descrito como el estudio sistemático de los procesos algorítmicos que describen y transforman información: su teoría, análisis, diseño, eficiencia, implementación, algoritmos sistematizados y aplicación.4​ En términos más específicos se trata del estudio sistemático de la factibilidad, estructura, expresión y mecanización de procedimientos metódicos (o algoritmos) que subyacen en la adquisición, representación, procesamiento, almacenamiento, comunicación y acceso a la información. La información puede estar codificada en forma de bits en una memoria de computadora, o en algún otro objeto, como los genes y proteínas en una célula biológica.5​\",\n","           \"Una avión de caza, avión de combate, avión de guerra, o simplemente caza,1​ es una aeronave militar diseñada fundamentalmente para la guerra aérea con otras aeronaves, en oposición a los bombarderos, que están diseñados principalmente para atacar objetivos terrestres mediante el lanzamiento de bombas. Los cazas son pequeños, veloces y de gran maniobrabilidad. Muchos cazas poseen capacidades secundarias de ataque a tierra, y algunos son de doble propósito para actuar como cazabombarderos, término también usado para nombrar a los aviones de ataque a tierra con capacidades de caza.\",\n","          \"La gramática es el estudio de las reglas y principios que gobiernan el uso de las lenguas y la organización de las palabras dentro de unas oraciones y otro tipo de constituyentes sintácticos. También se denomina así al conjunto de reglas y principios que gobiernan el uso de una lengua concreta; así, cada lengua tiene su propia gramática.A veces se restringe el uso del término gramática a las reglas y principios que definen la morfología y la sintaxis. Sin embargo, la separación de los niveles no es totalmente nítida por diversas razones, como que ciertas reglas gramaticales se realizan en el nivel fonético-fonológico, o que existen parámetros o criterios semánticos que sirven para decidir cuándo una determinada construcción se considera gramatical. Algunas tendencias de la lingüística moderna representan un regreso a los temas de la gramática tradicional desde nuevos puntos de vista\"\n","          \"Un volcán (del nombre del dios mitológico romano Vulcano) es una estructura geológica por la que emerge el magma que se divide en lava y gases provenientes del interior de la Tierra. El ascenso del magma ocurre en episodios de actividad violenta denominados erupciones, que pueden variar en intensidad, duración y frecuencia, desde suaves corrientes de lava hasta explosiones extremadamente destructivas. En ocasiones, los volcanes adquieren una forma cónica por la acumulación de material de erupciones anteriores. En la cumbre se encuentra su cráter o caldera.\",\n","          \"El punto de origen de un terremoto se denomina foco o hipocentro, a partir de allí se propaga en forma de ondas sísmicas. El punto de la superficie terrestre que se encuentra más cerca del hipocentro, donde alcanzan en primer lugar las ondas sísmicas se llama epicentro. Dependiendo de su magnitud y origen, un terremoto puede causar desplazamientos de la corteza terrestre, corrimientos de tierras, maremotos (o también llamados tsunamis) o actividad volcánica. Para medir la energía que fue liberada por un terremoto se emplean diversas escalas, entre ellas, la escala de Richter que es la más conocida y utilizada por los medios de comunicación.\",\n","          \"La palabra mar también se usa para indicar secciones más pequeñas del océano, en parte interiores, y para algunos grandes lagos salados, totalmente interiores, como el mar Caspio, el mar Muerto o el mar de Aral. Se habla entonces de mar cerrado o interior, aunque el término correcto sería el de lago endorreico.\",\n","          \"Rusia es el país que limita con mayor número de países, un total de dieciséis,n. 5​ y el que tiene las fronteras más extensas. Limita con los siguientes países (empezando por el noroeste y siguiendo el sentido antihorario): Noruega, Finlandia, Estonia, Letonia, Bielorrusia, Lituania,n. 6​ Polonia,n. 6​ Ucrania,n. 7​ Georgia,n. 8​ Azerbaiyán, Kazajistán, República Popular China, Mongolia y Corea del Norte. Tiene límites de aguas territoriales con varios de los anteriores, con Japón y con Estados Unidos (en concreto, con el estado de Alaska). Limita también con los Estados de reconocimiento limitado Abjasia y Osetia del Sur y con las repúblicas de Donetsk y Lugansk, reclamadas y parcialmente controladas por la propia Rusia. Sus costas están bañadas por el océano Glacial Ártico, el norte del océano Pacífico y mares interiores como el Báltico, el Negro y el Caspio.\" ]"],"metadata":{"id":"sG_gDvDqoUAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# identificar las 5 palabras claves del documento"],"metadata":{"id":"s8BCfhhdlzw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir la funcion\n","def cleanPLN(text, language='spanish'):\n","    # Pasar a minúsculas\n","    text = text.lower()\n","\n","    # Remover etiquetas HTML\n","    text = BeautifulSoup(text, 'html.parser').get_text()\n","\n","    # Remover caracteres especiales y puntuación\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    # Tokenizar\n","    tokens = word_tokenize(text)\n","\n","    # Eliminar stop words\n","    stop_words = set(stopwords.words(language))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Dejar palabras con más de 3 caracteres\n","    tokens = [word for word in tokens if len(word) > 3]\n","\n","    # Regresar la cadena de texto simplificada\n","    cleaned_text = ' '.join(tokens)\n","\n","    return cleaned_text"],"metadata":{"id":"nKCcNkp8oDio","executionInfo":{"status":"ok","timestamp":1692378955074,"user_tz":300,"elapsed":1,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["corpus = [cleanPLN(texto) for texto in corpus]"],"metadata":{"id":"nvHJSgnNpYNh","executionInfo":{"status":"ok","timestamp":1692378983685,"user_tz":300,"elapsed":329,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pm6lsA-hp_Gy","executionInfo":{"status":"ok","timestamp":1692379118236,"user_tz":300,"elapsed":9,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"c42e4349-5c4d-408d-95b9-934df801a103"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['matematicas ciencias antiguas florecio primero antiguedad mesopotamia cuanto geometria20 india china tarde antiguedad grecia helenismo data orientacion hacia tarea demostracion puramente logica primera axiomatizacion saber geometria euclidiana20 edad media sobrevivio forma independiente primer humanismo universidades mundo arabe',\n"," 'guerra forma conflicto sociopolitico grave grupos humanos sociedades tribales civilizadas grave ultimas complejas masificadas tecnificadas quizas antigua relaciones internacionales comienzo civilizaciones constata enfrentamiento organizado grupos humanos armados proposito controlar recursos naturales humanos conflictos cazadores nomadas recolectores sedentarios desarrollaron concepto propiedad3 exigir desarme imponer algun tipo tributo ideologia nacionalidad religion sometiendo despojando caso destruyendo enemigo tipo conducta gregaria extensible mayor parte hominidos4 encuentra estrechamente relacionado concepto etologico territorialidad',\n"," 'biologia ocupa descripcion caracteristicas comportamientos organismos individuales especies conjunto relaciones seres vivos interacciones entorno modo trata estudiar estructura dinamica funcional comunes seres vivos establecer leyes generales rigen vida organica principios esta6',\n"," 'ciencias computacion ciencias informatica ciencias formales abarcan bases teoricas informacion computacion aplicacion sistemas informaticos1 cuerpo conocimiento ciencias computacion frecuentemente descrito estudio sistematico procesos algoritmicos describen transforman informacion teoria analisis diseno eficiencia implementacion algoritmos sistematizados aplicacion4 terminos especificos trata estudio sistematico factibilidad estructura expresion mecanizacion procedimientos metodicos algoritmos subyacen adquisicion representacion procesamiento almacenamiento comunicacion acceso informacion informacion puede codificada forma bits memoria computadora algun objeto genes proteinas celula biologica5',\n"," 'avion caza avion combate avion guerra simplemente caza1 aeronave militar disenada fundamentalmente guerra aerea aeronaves oposicion bombarderos estan disenados principalmente atacar objetivos terrestres mediante lanzamiento bombas cazas pequenos veloces gran maniobrabilidad cazas poseen capacidades secundarias ataque tierra doble proposito actuar cazabombarderos termino tambien usado nombrar aviones ataque tierra capacidades caza',\n"," 'gramatica estudio reglas principios gobiernan lenguas organizacion palabras dentro unas oraciones tipo constituyentes sintacticos tambien denomina conjunto reglas principios gobiernan lengua concreta cada lengua propia gramaticaa veces restringe termino gramatica reglas principios definen morfologia sintaxis embargo separacion niveles totalmente nitida diversas razones ciertas reglas gramaticales realizan nivel foneticofonologico existen parametros criterios semanticos sirven decidir determinada construccion considera gramatical tendencias linguistica moderna representan regreso temas gramatica tradicional nuevos puntos vistaun volcan nombre dios mitologico romano vulcano estructura geologica emerge magma divide lava gases provenientes interior tierra ascenso magma ocurre episodios actividad violenta denominados erupciones pueden variar intensidad duracion frecuencia suaves corrientes lava explosiones extremadamente destructivas ocasiones volcanes adquieren forma conica acumulacion material erupciones anteriores cumbre encuentra crater caldera',\n"," 'punto origen terremoto denomina foco hipocentro partir alli propaga forma ondas sismicas punto superficie terrestre encuentra cerca hipocentro alcanzan primer lugar ondas sismicas llama epicentro dependiendo magnitud origen terremoto puede causar desplazamientos corteza terrestre corrimientos tierras maremotos tambien llamados tsunamis actividad volcanica medir energia liberada terremoto emplean diversas escalas escala richter conocida utilizada medios comunicacion',\n"," 'palabra tambien indicar secciones pequenas oceano parte interiores grandes lagos salados totalmente interiores caspio muerto aral habla entonces cerrado interior aunque termino correcto seria lago endorreico',\n"," 'rusia pais limita mayor numero paises total dieciseisn fronteras extensas limita siguientes paises empezando noroeste siguiendo antihorario noruega finlandia estonia letonia bielorrusia lituanian polonian ucranian georgian azerbaiyan kazajistan republica popular china mongolia corea norte limites aguas territoriales varios anteriores japon unidos concreto alaska limita tambien reconocimiento limitado abjasia osetia republicas donetsk lugansk reclamadas parcialmente controladas propia rusia costas estan banadas oceano glacial artico norte oceano pacifico mares interiores baltico negro caspio']"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["tfidf = TfidfVectorizer(min_df=1)\n","X = tfidf.fit_transform(corpus)"],"metadata":{"id":"bWZe706dpnjb","executionInfo":{"status":"ok","timestamp":1692379493775,"user_tz":300,"elapsed":184,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["for i in range(len(corpus)):\n","  U = pd.DataFrame(X.toarray()[i],\n","                   index = tfidf.get_feature_names_out,\n","                   columns=[\"TF+iDF\"])\n","  print(\"Documento\",i)\n","  print(U.sort_values(\"TF+iDF\")[::-1].head(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"kNFyYQ3XqSYq","executionInfo":{"status":"error","timestamp":1692379479098,"user_tz":300,"elapsed":190,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"1e3ff30e-55a9-42f4-f8d3-815d710d3ce1"},"execution_count":69,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-c4ceb03b9bcc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   U = pd.DataFrame(X.toarray()[i],\n\u001b[0m\u001b[1;32m      3\u001b[0m                    \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    columns=[\"TF+iDF\"])\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Documento\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     index, columns = _get_axes(\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_get_axes\u001b[0;34m(N, K, index, columns)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".*the Index constructor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_multi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dtype_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0;31m# with e.g. a list [1, 2, 3] casting to numeric is _not_ deprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36masarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__array__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"]}]}]}