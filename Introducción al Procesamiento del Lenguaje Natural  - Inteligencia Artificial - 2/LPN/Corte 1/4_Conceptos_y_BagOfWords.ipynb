{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<table>\n","    <tr>\n","        <td><img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/1485763/images/9060062/Header.png\" width=\"300\"/></td>\n","        <td>&nbsp;</td>\n","        <td>\n","            <h1 style=\"font-size:200%;color:blue;text-align:center\">    <FONT COLOR=\"blue\">  Conceptos  </p>  Bag of Words  </FONT>         </h1></td>         \n","        <td>\n","            <tp><p style=\"font-size:99%;text-align:center\">PLN </p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">Pregrado MACC 2023-2</p></tp>\n","            <tp><p style=\"font-size:115%;text-align:center\">Prof. Fabián Sánchez</p></tp>\n","        </td>\n","    </tr>\n","</table>"],"metadata":{"id":"RY1SVRFYpmrO"}},{"cell_type":"markdown","source":["A continuación revisaremos rápidamente algunos conceptos claves en el desarrollo de procesamiento de lenguaje natural."],"metadata":{"id":"mMK2AqI_fnMX"}},{"cell_type":"markdown","source":["<FONT SIZE=3 COLOR=\"blue\"> **Normalización** </FONT> Es una etapa en el Procesamiento de Lenguaje Natural donde se busca poner todo el texto en igualdad de condiciones, es decir, todo en el mismo tipo de letra , eliminar signos de puntuación, quitar palabras, etc. En otras palabras limpiamos el texto para aplicar algunos modelo o técnicas al mismo.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Tokenización** </FONT>: Es el proceso de dividir las cadenas de texto de un documento en piezas más pequeñas que se denominan *tokens*.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Stemming:** </FONT>: Es la tarea en procesamiento de lenguaje natural de eliminar: sufijos, prefijos, etc. para obtener el tallo de la palabra:\n","\n","- Caminando : caminar\n","\n","- Anteriormente: anterior\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Lematización:** </FONT> es el proceso en el que se identifica y sustituye una palabra (plurales, femeninos, conjugaciones, etc) por su *lema* (raíz). Esto quiere decir por una palabra qe sea válida en el idioma.\n","\n","- Carros : Carro\n","\n","- Comiendo : comer\n","\n","- Entrenaría : entrenar\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Part of Speech (PoS)-Tagging:** </FONT> Proceso mediante el cual se asigna una etiqueta de categoría a las partes tokenizadas de una oración. Es decir, si es sustantivo, verbo, etc.\n","\n","    - Artículo o determinante: un, el, las\n","    - Sustantivo o nombre: hombre, tren, carro, mueble\n","    - Pronombre: yo, tu, el, ella  \n","    - Verbo: correr, pensar --acción\n","    - Adjetivo: alto, valente, nuevo.\n","    - Adverbio: tarde, temprano, anoche, constantemente.\n","    - Preposición: a, ante, con , contra, (unir palabras)\n","    - Conjunción: y, o, ni, que\n","    - Interjección: ¡ay! ,¡oh! (sentimiento)\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **n-grammas:** </FONT> modelo de representación para simplificar los contenidos de un texto. En los n-gramas nos interesa preservar secuencias contiguas de N-elementos de la selección de texto.\n","\n","<FONT SIZE=3 COLOR=\"blue\"> **Stop Words:** </FONT> Son palabras que no aportan al significado de la oraciones : preprosiciones, conjunciones, adverbios, artículos, etc."],"metadata":{"id":"hC3PolDHfOJV"}},{"cell_type":"markdown","source":["Veamos algunos ejemplos de lo anterior. Para ello vamos a importar la librería *nltk*"],"metadata":{"id":"7TwggYwAgeG2"}},{"cell_type":"code","source":["import nltk\n","nltk.download(\"all\")"],"metadata":{"id":"IoUT4-sUgnUh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204408162,"user_tz":300,"elapsed":91459,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"ddb022b5-a771-4106-cbaa-8ea5d4cfa4b4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 1. tokenizar </FONT>"],"metadata":{"id":"Jb-ogq0mg1tT"}},{"cell_type":"code","source":["# separa cuando encuentra espacio\n","documento1 = \"Cien años! de sole.dad 100.56 es una novela del escritor colombiano\\\n","Gabriel García Márquez, ganador del Premio Nobel de Literatura en 1982.\\\n","Es considerada una obra maestra? de la literatura hispanoamericana y universal.\\\n","Así como una de las obras más traducidas y leídas en español.\"\n","words = nltk.word_tokenize(documento1)\n","print(words)"],"metadata":{"id":"FyP4NI22g3Zf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204415312,"user_tz":300,"elapsed":9,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"76e96ab6-e461-47a2-b43c-43805e95c7af"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Cien', 'años', '!', 'de', 'sole.dad', '100.56', 'es', 'una', 'novela', 'del', 'escritor', 'colombianoGabriel', 'García', 'Márquez', ',', 'ganador', 'del', 'Premio', 'Nobel', 'de', 'Literatura', 'en', '1982.Es', 'considerada', 'una', 'obra', 'maestra', '?', 'de', 'la', 'literatura', 'hispanoamericana', 'y', 'universal.Así', 'como', 'una', 'de', 'las', 'obras', 'más', 'traducidas', 'y', 'leídas', 'en', 'español', '.']\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 2. Stemming </FONT>\n","\n","- Funciona mejor para inglés"],"metadata":{"id":"_Iju0lxnhhG1"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","stm = PorterStemmer()"],"metadata":{"id":"zbtW-IwghqVG","executionInfo":{"status":"ok","timestamp":1692204419306,"user_tz":300,"elapsed":3,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(stm.stem('running'))\n","print(stm.stem('minimum'))\n","print(stm.stem('likely'))"],"metadata":{"id":"qBIMPiFJhsKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204421736,"user_tz":300,"elapsed":7,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"ffad93d6-3402-4246-cc7e-7e30b246fb94"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["run\n","minimum\n","like\n"]}]},{"cell_type":"markdown","source":["Con la siguiente librería podemos usar otros idiomas. Pero dado que *nltk* está enfocado al inglés, funciona mejor con estos idiomas."],"metadata":{"id":"9UzQtKIhiSjn"}},{"cell_type":"code","source":["from nltk.stem.snowball import SnowballStemmer"],"metadata":{"id":"zneGsAsviFZq","executionInfo":{"status":"ok","timestamp":1692204431558,"user_tz":300,"elapsed":361,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(\" \".join(SnowballStemmer.languages))"],"metadata":{"id":"ut8AHFiFiDB6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204434291,"user_tz":300,"elapsed":5,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"036bfab4-b6f5-44aa-98a7-537eddeb312b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"]}]},{"cell_type":"code","source":["stemmer = SnowballStemmer(\"spanish\")"],"metadata":{"id":"HQSBgn_qiI08","executionInfo":{"status":"ok","timestamp":1692204437112,"user_tz":300,"elapsed":4,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(stemmer.stem(\"precisamente\"))\n","print(stemmer.stem(\"increible\"))\n"],"metadata":{"id":"LRe-zkv-iKhO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204439007,"user_tz":300,"elapsed":6,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"1857b2d2-0343-4aa1-c4f4-1e29aa2dfcef"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["precis\n","increibl\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 3. Stop Words </FONT>\n","\n"],"metadata":{"id":"Smm70vUuidoP"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","print(set(stopwords.words('spanish')))"],"metadata":{"id":"mjCC5SkNijmo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204448069,"user_tz":300,"elapsed":728,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"f36eaadb-d705-4f58-ba8b-005c3ba3082a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'otra', 'esta', 'estar', 'está', 'sería', 'hayáis', 'estemos', 'ni', 'esa', 'tendrá', 'habidas', 'estuviéramos', 'habríamos', 'vuestra', 'mi', 'éramos', 'son', 'teníamos', 'estuvieras', 'estados', 'estuvieseis', 'sus', 'estabas', 'tuyos', 'qué', 'hasta', 'tuviésemos', 'nosotros', 'donde', 'estado', 'te', 'tus', 'hubierais', 'le', 'estuviesen', 'estada', 'hubiste', 'eran', 'fuesen', 'habrás', 'ha', 'habré', 'tuve', 'tendremos', 'fueses', 'teníais', 'todo', 'siente', 'tenían', 'habiendo', 'hubisteis', 'será', 'sean', 'estarán', 'estuvieran', 'están', 'mías', 'hubiese', 'sentidos', 'sobre', 'eras', 'hayamos', 'tuviera', 'de', 'tendrás', 'tuviéramos', 'habremos', 'estuviésemos', 'pero', 'seréis', 'ese', 'tenías', 'habidos', 'tuvieran', 'algunas', 'tuvieras', 'hubiera', 'ante', 'el', 'había', 'es', 'has', 'serías', 'suya', 'tuyo', 'mí', 'muchos', 'erais', 'seremos', 'cual', 'fui', 'estuvieron', 'estuvisteis', 'esas', 'estaría', 'a', 'tuvisteis', 'hubieras', 'estamos', 'tendrán', 'os', 'tienen', 'seríais', 'la', 'suyo', 'hubieseis', 'tuviste', 'hubiesen', 'tendré', 'nos', 'tuvo', 'estuvimos', 'haya', 'tened', 'tenida', 'los', 'antes', 'seríamos', 'muy', 'habido', 'estaremos', 'tienes', 'ya', 'estuvieses', 'teniendo', 'otras', 'estuviste', 'soy', 'tenga', 'tendréis', 'hubimos', 'tengan', 'estés', 'vuestros', 'sois', 'habrían', 'hubieran', 'para', 'tengas', 'mucho', 'estaríais', 'porque', 'tenéis', 'mía', 'estaba', 'serían', 'míos', 'habíamos', 'tiene', 'les', 'estaban', 'fuerais', 'habéis', 'tuvimos', 'sentida', 'las', 'hubiéramos', 'tendrías', 'y', 'habréis', 'fuese', 'quien', 'fueseis', 'habrá', 'me', 'fuésemos', 'sintiendo', 'vosotras', 'este', 'fueran', 'fuera', 'tenemos', 'hay', 'en', 'tengamos', 'seré', 'poco', 'ti', 'tenido', 'por', 'como', 'otro', 'seamos', 'contra', 'ellos', 'estaríamos', 'sea', 'unos', 'estad', 'habida', 'su', 'que', 'yo', 'eso', 'seáis', 'nuestra', 'e', 'suyas', 'habías', 'tendríamos', 'esté', 'estarías', 'estaréis', 'serás', 'ellas', 'también', 'vuestro', 'suyos', 'tuya', 'nada', 'hubieron', 'tenidos', 'estábamos', 'entre', 'tuyas', 'esos', 'él', 'tendrían', 'estuviese', 'tengo', 'estarás', 'esto', 'estuviera', 'nuestras', 'habría', 'tu', 'sí', 'tenía', 'habíais', 'tenidas', 'vuestras', 'una', 'con', 'habríais', 'algo', 'mis', 'habrías', 'tuvieseis', 'serán', 'habían', 'o', 'sentid', 'al', 'era', 'fuéramos', 'otros', 'estos', 'nuestros', 'del', 'ella', 'tú', 'fueron', 'estas', 'fueras', 'hemos', 'tanto', 'estéis', 'tendría', 'se', 'estarían', 'sin', 'nuestro', 'sentido', 'lo', 'quienes', 'estoy', 'tuvieses', 'hubo', 'durante', 'sentidas', 'estando', 'cuando', 'fuiste', 'han', 'estadas', 'más', 'hayas', 'estuvo', 'habrán', 'he', 'uno', 'tuviesen', 'nosotras', 'estará', 'desde', 'hayan', 'hube', 'fuisteis', 'estabais', 'tuviese', 'hubiésemos', 'fuimos', 'estás', 'vosotros', 'estén', 'mío', 'todos', 'seas', 'estáis', 'un', 'fue', 'estuve', 'somos', 'tendríais', 'no', 'eres', 'estaré', 'tengáis', 'tuvieron', 'hubieses', 'algunos', 'estuvierais', 'tuvierais'}\n"]}]},{"cell_type":"code","source":["doc = \"Un radar multa a Fabian Sanchez por conducir demasiado rapido en la autopista\"\n","words = nltk.word_tokenize(doc)\n","for word in words:\n","        if word in stopwords.words('spanish'):\n","            print (word)"],"metadata":{"id":"dGTsiZL9imHS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204449982,"user_tz":300,"elapsed":8,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"e7aeb501-666f-4e4f-fdd0-a63ddb4f09e4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["a\n","por\n","en\n","la\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 4. Lematización </FONT>"],"metadata":{"id":"u6-l6vAHio1i"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","lemm = WordNetLemmatizer()"],"metadata":{"id":"czyOecb1iub-","executionInfo":{"status":"ok","timestamp":1692204456335,"user_tz":300,"elapsed":438,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(lemm.lemmatize('dogs'))\n","print(lemm.lemmatize('houses'))\n","print(lemm.lemmatize('perros'))\n"],"metadata":{"id":"0somr_U0iwA6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204460718,"user_tz":300,"elapsed":1552,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"8e512b61-8de0-45d3-8df4-ee3888bfba67"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["dog\n","house\n","perros\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=4 COLOR=\"purple\"> 5. n-grams </FONT>\n","\n","Los **n-gram** son un modelo de representación que selecciona secuencias contiguas de *n*-elementos del texto\n","\n","- Veamos un ejemplo"],"metadata":{"id":"osOlGHB_3q5y"}},{"cell_type":"code","source":["from nltk import ngrams"],"metadata":{"id":"KZhKl1fz4AiO","executionInfo":{"status":"ok","timestamp":1692204472094,"user_tz":300,"elapsed":846,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["doc = \"Un radar multa a Fabian Sanchez por conducir demasiado rapido en la autopista\"\n","words = nltk.word_tokenize(doc)\n","num_elementos = 4\n","n_grams = ngrams(words, num_elementos)\n","for grams in n_grams:\n","    print (grams)"],"metadata":{"id":"Q4GNZSEtjKYd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204479235,"user_tz":300,"elapsed":11,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"4bcefbde-050a-489c-937f-6fb4c5c64fe8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["('Un', 'radar', 'multa', 'a')\n","('radar', 'multa', 'a', 'Fabian')\n","('multa', 'a', 'Fabian', 'Sanchez')\n","('a', 'Fabian', 'Sanchez', 'por')\n","('Fabian', 'Sanchez', 'por', 'conducir')\n","('Sanchez', 'por', 'conducir', 'demasiado')\n","('por', 'conducir', 'demasiado', 'rapido')\n","('conducir', 'demasiado', 'rapido', 'en')\n","('demasiado', 'rapido', 'en', 'la')\n","('rapido', 'en', 'la', 'autopista')\n"]}]},{"cell_type":"markdown","source":["# ***Hacer la función de ngrams en python como pequeñas tareas***"],"metadata":{"id":"jnlIy8zkPKzi"}},{"cell_type":"markdown","source":["<FONT SIZE=4 COLOR=\"purple\"> 6. Etiquetado Formológico- PoS (part of speech-tagging) </FONT>\n","\n","NLTK nos da algunas herramientas para crear etiquetadores morfológicos, es decir, identificar si una palabra es verbo, sustantivo, adjetivo , etc.\n","\n","CC : coodinate conjunction\n","\n","CD : cardinal number\n","\n","DT : determiner\n","\n","EX ; existencial there\n","\n","FW : foeign word\n","\n","IN : preprosition or subordinating\n","\n","JJ : adjetive\n","\n","JJR : adjetive, comparative\n","\n","JJS : adjetive superlative\n","...\n","BV : verbo forma base\n","\n","VBD : verbo pasado\n","\n","VBG : verb gerundio\n","\n","Para mayor información ver\n","\n","[TAGGING](https://pythonspot.com/nltk-speech-tagging/)\n","\n","Usaremos **nltk.pos_tag** que es un etiquetador morfológico basado en aprendizaje automático. A partir de miles de ejemplos de oraciones etiquetadas manualmente, el sistema *ha aprendido*, calculando frecuencias y generalizando cuál es la categoría gramatical más probable para cada token."],"metadata":{"id":"xDq2n4pQjtZN"}},{"cell_type":"code","source":["oracion1 = \"This is the lost dog I found at the park\".split()\n","oracion2 = \"The progress of the humankind as I progress\".split()\n","oracion3 = \"When I went to the University, I played soccer\".split()\n","print(nltk.pos_tag(oracion1))\n","print(nltk.pos_tag(oracion2))\n","print(nltk.pos_tag(oracion3))"],"metadata":{"id":"1MqZiYnLkFEI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204522569,"user_tz":300,"elapsed":8,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"bcfb18e0-b70f-4fe8-d024-d6fe91157061"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('lost', 'JJ'), ('dog', 'NN'), ('I', 'PRP'), ('found', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('park', 'NN')]\n","[('The', 'DT'), ('progress', 'NN'), ('of', 'IN'), ('the', 'DT'), ('humankind', 'NN'), ('as', 'IN'), ('I', 'PRP'), ('progress', 'VBP')]\n","[('When', 'WRB'), ('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('University,', 'NNP'), ('I', 'PRP'), ('played', 'VBD'), ('soccer', 'NN')]\n"]}]},{"cell_type":"markdown","source":["<FONT SIZE=4 COLOR=\"red\"> Observación: </FONT>\n","\n","### Etiquetador basado en Expresiones Regulares\n","\n","- Las expresiones regulares nos permiten especificar cadenas de texto.\n","\n","- Esta nos permite identificar patrones en las palabras para poderlas clasificar\n","\n","- Vamos a crear nuestra propia lista para etiquetar. (tagging)\n","\n","- El primer elemento es la expresión regular y la segunda la categoría gramatical.\n","\n","- Usamos la instrucción **RegexpTagger**.\n","\n","- Este ejemplo lo haremos en inglés\n","\n","Usaremos Raw String Notation *(r\"texto\")* : una forma en la cual Python nos permite implementar expresiones regulares.\n","\n"],"metadata":{"id":"JfH19KRUkOIH"}},{"cell_type":"markdown","source":["Veamos un par de ejemplos"],"metadata":{"id":"1KSy7wRumOy5"}},{"cell_type":"code","source":["pattern = [(r\"He\", \"TAG\")]\n","tagger=nltk.RegexpTagger(pattern)\n","\n","print(tagger.tag(\"He was born He in March 1991\".split()))"],"metadata":{"id":"K7sYEv9fkg87","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204657599,"user_tz":300,"elapsed":481,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"f0bf2d80-1882-4923-8d10-1f8301d5f733"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[('He', 'TAG'), ('was', None), ('born', None), ('He', 'TAG'), ('in', None), ('March', None), ('1991', None)]\n"]}]},{"cell_type":"code","source":["patrones = [(r\"[Aa]m$\", \"Verb-to-be\"),\n","            (r\".*ly$\", \"adverb\"),\n","            (r\"[Ww]ere\", \"Verb-to-be-past\"),\n","            (r\"[Tt]hey$\", \"pron\"),\n","            (r\".*ing$\", \"gerun\"),\n","            (r'.*ed$', 'simple past')]\n","tagger=nltk.RegexpTagger(patrones)\n","print(tagger.tag(\"They were playing soccer fastly . I am happy\".split()))"],"metadata":{"id":"pV4qWQm3mWqK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692204730737,"user_tz":300,"elapsed":324,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"e13dba34-6e5f-434c-a65a-73e1f9441e8d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[('They', 'pron'), ('were', 'Verb-to-be-past'), ('playing', 'gerun'), ('soccer', None), ('fastly', 'adverb'), ('.', None), ('I', None), ('am', 'Verb-to-be'), ('happy', None)]\n"]}]},{"cell_type":"code","source":["print(nltk.pos_tag(\"They were playing soccer fastly . I am happy\".split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdtuVvsxRfhb","executionInfo":{"status":"ok","timestamp":1692204950289,"user_tz":300,"elapsed":425,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"6ccb7ab1-cc47-4306-a596-0f265b989b6c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[('They', 'PRP'), ('were', 'VBD'), ('playing', 'VBG'), ('soccer', 'NN'), ('fastly', 'RB'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('happy', 'JJ')]\n"]}]},{"cell_type":"markdown","source":["# <FONT SIZE=5 COLOR=\"purple\"> Bag of Words </FONT>\n","\n","- *Bag of Words* o ***Bolsa de Palabras*** (***BoW***) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto (o corpus).\n","\n"],"metadata":{"id":"5QdN1EcOVFpZ"}},{"cell_type":"code","source":["documento = [\"carlos juega mucho carlos carlos carro carro juega universidad universidad juega juega  mucho mucho carlos\",\n","             \"conduce conduce conduce carro carro mucho mucho carro carro carro carro carro\",\n","             \"universidad universidad MACC MACC MACC MACC MACC MACC UR UR UR UR UR UR UR\",\n","             \"universidad universidad universidad carlos carlos conduce conduce carlos carlos carlos UR UR UR UR\",\n","             \"universidad universidad carlos carlos carlos conduce conduce carro carro carro carro mucho mucho mucho\",\n","             \"carlos juega universidad carro mucho juega mucho\"]"],"metadata":{"id":"QL2mjkNoVNIm","executionInfo":{"status":"ok","timestamp":1692206602719,"user_tz":300,"elapsed":363,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["A continuación creamos una bolsa de palabras con las frecuencias."],"metadata":{"id":"ovXnk5-wrjrG"}},{"cell_type":"code","source":["# Construimos la bolsa de palabras\n","bow = dict()\n","for doc in documento:\n","    doc = doc.split(\" \")\n","    for palabra in doc:\n","        if palabra in bow:\n","            bow[palabra] += 1\n","        else:\n","            bow[palabra] = 1\n","print(bow)"],"metadata":{"id":"AxgKnaPwVOBm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692206604677,"user_tz":300,"elapsed":375,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"2208bb34-6063-4c48-9d3c-897552b4fed6"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 13, 'juega': 6, 'mucho': 10, 'carro': 14, 'universidad': 10, '': 1, 'conduce': 7, 'MACC': 6, 'UR': 11}\n"]}]},{"cell_type":"markdown","source":["**Ejercicio en clase:**\n","\n","Utilice la librería ***nltk*** para generar en una línea de código el resultado anterior."],"metadata":{"id":"dy8QO178rxp8"}},{"cell_type":"code","source":["from nltk import FreqDist\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"p5rAI50ZYV9r","executionInfo":{"status":"ok","timestamp":1692206819304,"user_tz":300,"elapsed":355,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["FreqDist([palabra for doc in documento for palabra in word_tokenize(doc)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgq3MOIEY-GS","executionInfo":{"status":"ok","timestamp":1692206989873,"user_tz":300,"elapsed":409,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"b8f2b616-e04e-4323-ca1c-0b138340c894"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'carro': 14, 'carlos': 13, 'UR': 11, 'mucho': 10, 'universidad': 10, 'conduce': 7, 'juega': 6, 'MACC': 6})"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["Existen más formas de construir bolsas de palabras. Las más utilizadas son las siguiente:\n","\n","  1. Vectores de Frecuencias (TF)\n","  2. One-Hot-Encode\n","  3. Term Frequency-Inverse Document Frequency (TF-IDF)\n","    \n","Vamos a abordar cada una de las anteriores formas y utilizaremos las siguientes librerías para su implementación.\n","\n","  * ***scikit***\n","  * ***NLTK***\n","  * ***Gensim***\n","\n","¿Por qué es necesario conocer las tres?\n","\n","La respuesta a esta pregunta radica en que hay librerías especializadas para determinados modelos, y en general, debemos hacer una bolsa de palabras antes de aplicar algunos modelos. Por ejemplo, *Gensim* es muy usada para toping-modeling y LDA, etc.\n","    \n","\n","\n","\n","### -Scikit\n","\n","* Esta librería devuelve una matriz en la que las **filas representan a cada documento del corpus** y las **columnas el número de apariciones de las palabras**.\n","\n","\n","* Para saber que palabra corresponde a cada columan de la matriz, scikit nos devuelve una lista en la que coinciden los indice de cada una de las palabras de la lista con la matriz.\n","\n","\n","* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"],"metadata":{"id":"sG5QEPy3VVAz"}},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 1. (TF) Vectores de Frecuencias </FONT>\n","\n","- Los ***vectores de frecuencias (TF)*** es el método más sencillo de construir las ***Bolsas de Palabras***.\n","\n","- Simplemente consiste en contar cuantas veces aparece una palabra en el documento del corpus."],"metadata":{"id":"BUyQz6q0svag"}},{"cell_type":"code","source":["# para hacer TF en sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","vectors = vectorizer.fit_transform(documento)\n","\n","# Resultados\n","print(vectorizer.get_feature_names_out())\n","print(vectors.toarray())"],"metadata":{"id":"gRzadR4YVXJB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692207258714,"user_tz":300,"elapsed":423,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"77a812d4-b601-4518-99e5-d605756246b6"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["['carlos' 'carro' 'conduce' 'juega' 'macc' 'mucho' 'universidad' 'ur']\n","[[4 2 0 4 0 3 2 0]\n"," [0 7 3 0 0 2 0 0]\n"," [0 0 0 0 6 0 2 7]\n"," [5 0 2 0 0 0 3 4]\n"," [3 4 2 0 0 3 2 0]\n"," [1 1 0 2 0 2 1 0]]\n"]}]},{"cell_type":"markdown","source":["### Gensim\n","\n","* ***Gensim trabaja con \"Diccionarios\"*** (gensim.corpora.Dictionary) que es una estructura de datos en la que guarda el orden de las palabras que hay en el corpus.\n","\n","\n","* Posteriormente se construye la Bolsa de Palabras con las frecuencias con la función \"***doc2bow***\"\n","\n","\n","* Como resultado devuelve una lista por cada documento en la que se indicida por cada palabra del documento identificada por el 'id' del diccionario previamente construido la frecuencia de esa palabra en el documento.\n","\n","\n","* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/corpora/dictionary.html"],"metadata":{"id":"rQ5nQYhjVewC"}},{"cell_type":"code","source":["import gensim\n","# primero tokenizamos\n","tokenize = [nltk.word_tokenize(text) for text in documento]\n","# creamos un diccionario\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","# utilizamos doc2bow\n","vectors = [dictionary.doc2bow(token) for token in tokenize]\n","\n","# codigos de las palabras\n","print(dictionary.token2id)\n","# frecuencias\n","print('\\nApariciones de las palabras en los documentos:')\n","vectors"],"metadata":{"id":"D7DHzy2_VguA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692207271813,"user_tz":300,"elapsed":881,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"14657daa-2340-4150-935e-415679295d9e"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 0, 'carro': 1, 'juega': 2, 'mucho': 3, 'universidad': 4, 'conduce': 5, 'MACC': 6, 'UR': 7}\n","\n","Apariciones de las palabras en los documentos:\n"]},{"output_type":"execute_result","data":{"text/plain":["[[(0, 4), (1, 2), (2, 4), (3, 3), (4, 2)],\n"," [(1, 7), (3, 2), (5, 3)],\n"," [(4, 2), (6, 6), (7, 7)],\n"," [(0, 5), (4, 3), (5, 2), (7, 4)],\n"," [(0, 3), (1, 4), (3, 3), (4, 2), (5, 2)],\n"," [(0, 1), (1, 1), (2, 2), (3, 2), (4, 1)]]"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 2. (TF) One-Hot-Encode </FONT>\n","\n","- Este método de construcción de la Bolsa de Palabras consiste en indicar con un flag ([0,1], [True, False], etc.) si una palabra aparece o no en el documento.\n","\n","Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer"],"metadata":{"id":"2y0NmXJ4Vi_A"}},{"cell_type":"code","source":["# para genetar one-hot.encode con sklearn\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import Binarizer\n","\n","vectorizer = CountVectorizer()\n","corpus = vectorizer.fit_transform(documento)\n","\n","onehot = Binarizer()\n","corpus = onehot.fit_transform(corpus.toarray())\n","\n","# Resultados\n","print(vectorizer.get_feature_names_out())\n","print(corpus)"],"metadata":{"id":"YdfIk5xKVkM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692207278140,"user_tz":300,"elapsed":419,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"a14dca10-9a12-4dd2-e0dc-d7beefc2fcf0"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["['carlos' 'carro' 'conduce' 'juega' 'macc' 'mucho' 'universidad' 'ur']\n","[[1 1 0 1 0 1 1 0]\n"," [0 1 1 0 0 1 0 0]\n"," [0 0 0 0 1 0 1 1]\n"," [1 0 1 0 0 0 1 1]\n"," [1 1 1 0 0 1 1 0]\n"," [1 1 0 1 0 1 1 0]]\n"]}]},{"cell_type":"code","source":["# Para hacerlo en nltk\n","import nltk\n","\n","def tokenize(text):\n","    for token in nltk.word_tokenize(text):\n","        yield token\n","\n","def vectorize(corpus):\n","    return {token: True for token in tokenize(corpus)}\n","\n","vectors = map(vectorize, documento)\n","\n","# Resultados\n","for v in vectors:\n","    print(v)"],"metadata":{"id":"NoQFSzGoVns2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692207327963,"user_tz":300,"elapsed":367,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"868b493b-4a7d-4aa9-d91a-9b7f21ae8add"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': True, 'juega': True, 'mucho': True, 'carro': True, 'universidad': True}\n","{'conduce': True, 'carro': True, 'mucho': True}\n","{'universidad': True, 'MACC': True, 'UR': True}\n","{'universidad': True, 'carlos': True, 'conduce': True, 'UR': True}\n","{'universidad': True, 'carlos': True, 'conduce': True, 'carro': True, 'mucho': True}\n","{'carlos': True, 'juega': True, 'universidad': True, 'carro': True, 'mucho': True}\n"]}]},{"cell_type":"markdown","source":["**Nota:** En Gensim, la estructura de salida es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos solo tendrán valor $1$.\n","\n","Para más información ver el siguiente enlace:\n","\n","https://radimrehurek.com/gensim/corpora/dictionary.html"],"metadata":{"id":"Bwvl9cOiVqjM"}},{"cell_type":"code","source":["import gensim\n","\n","tokenize = [nltk.word_tokenize(text) for text in documento]\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in tokenize]\n","\n","# codificación\n","print(dictionary.token2id)\n","# bolsa de palabras\n","print('\\nApariciones de las palabras en los documentos (id, 1):')\n","vectors"],"metadata":{"id":"6UK9T9KoVrzU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692207636200,"user_tz":300,"elapsed":429,"user":{"displayName":"Laura Gonzalez","userId":"04681677792797859565"}},"outputId":"9e94971d-3a98-4cc9-cbe5-472e63795b8e"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["{'carlos': 0, 'carro': 1, 'juega': 2, 'mucho': 3, 'universidad': 4, 'conduce': 5, 'MACC': 6, 'UR': 7}\n","\n","Apariciones de las palabras en los documentos (id, 1):\n"]},{"output_type":"execute_result","data":{"text/plain":["[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n"," [(1, 1), (3, 1), (5, 1)],\n"," [(4, 1), (6, 1), (7, 1)],\n"," [(0, 1), (4, 1), (5, 1), (7, 1)],\n"," [(0, 1), (1, 1), (3, 1), (4, 1), (5, 1)],\n"," [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## <FONT SIZE=5 COLOR=\"blue\"> 3. Term Frequency-Inverse Document Frequency (TF-IDF) </FONT>\n","\n","- El TF-IDF (Frecuencia de Termino - Frecuencia Inversa de Documento) es una medida numérica que permite expresar como de relevante es una palabra para un documento en una colección de documentos (o corpus).\n","\n","- La fórmula que se aplica es\n","\n","$$idf(t_i) = \\log \\left (\\dfrac{N}{n_i} \\right)$$\n","\n","donde\n","\n","  - $t_i$ es el término $i$.\n","  - $n_i$ es número de documentos donde aparace el termino $i$.\n","  - $N$ número de documentos.\n","\n","\n","## <FONT SIZE=3 COLOR=\"blue\"> Motivación de TF+iDF </FONT>\n","\n","- **Referencia de consulta:** Understanding Inverse Document Frequency:\n","On theoretical arguments for IDF. Stephen Robertson. Microsoft Research.\n","\n","- En el campo de la información, la **entropía** (Shannon) mide la incertidumbre de una fuente de información.\n","\n","- También se puede decir, que es la información promedio que contienen los símbolos usados.\n","\n","- En ese orden de ideas, las palabras que son más frecuentes aportan menos información. (el, a de, que). Si uno las quita de un documento, podría entender más de este que si quita una palabra que se repita menos.\n","\n","- La ley de **Zipf**: es una regla empírica que indica que la frecuencia de aparición de distintas palabras sigue una distribución que es aproximadamente\n","\n","$$P_n \\approx \\dfrac{1}{a^n}$$\n","\n","donde, $P_n$ es la frecuencia de la n-ésima palabra más frecuente y $a>1$.\n","\n","- La entropía termodinámica es igual a la entropía de la teoría de la información de esa distribución (medida usando el logaritmo neperiano) multiplicada por la constante de Boltzmann k. $k\\log N$.\n","\n","- Supongamos que tenemos un evento con grado de indeterminación $k$ o en otras palabras, que existen k estados posibles y todos son equiprobables. Luego\n","\n","$$probabilidad(ocurrencia) = \\dfrac{1}{k}$$.\n","\n","$$P(t_i)= \\dfrac{n_i}{N}$$\n","$$idf(t_i) = - \\log (P(t_i)) = \\log (\\dfrac{N}{n_i})$$\n","\n","**jercicio en clase:**\n","\n","Calcular $$idf(t_1 \\, y \\, t_2)= -log(P(t_1yt_2))=-log(P(t_1)P(t_2))$$\n","$$=-log(P(t_1))-log(P(t_2))=idf(t_1)+idf(t_2)$$\n","\n","* Construir la Bolsa de Palabras con TF-IDF en vez de con frecuencias evita dar \"importancia\" a texto muy largos y con mucha repetición de palabras, frente a textos cortos y con pocas repeticiones de palabras.\n","\n","También se puede definir de la siguiente manera\n","\n","* La media de ***TF-IDF*** tiene dos componentes que son:\n","    \n","    * ***TF*** (Term Frecuency): Es la frecuencia con la que aparece la palabra en un documento del corpus. Esta se define como:\n","    \n","    $$tf(t,d) = 1 + log(f_{t,d})$$\n","    \n","    * ***IDF*** (Inverse Document Frequency): La frecuencia inversa del documento nos indica lo común que es una palabra en el corpus.\n","    \n","    $$idf(t,D) = log(1 + \\frac{N}{n_t})$$\n","    \n","\n","* ***TF-IDF*** queda definido como:\n","$$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)$$\n","\n","\n","\n","### Ejemplos:\n","\n","* Veamos un ejemplo dado el siguiente corpus de 2 documentos con las siguientes palabras:\n","\n","```\n","corpus = [\"messi messi messi ronaldo ronaldo balon\",\n","          \"messi ronaldo futbol futbol futbol\"]\n","```\n","\n","* ***Ejemplo 1***: Calculamos el ***tf-idf*** de la palabra \"**messi**\" para el documento 1:\n","    \n","    * ***TF***:\n","        - t: número de veces que aparece la palabra \"messi\" en el documento 1 -> 3\n","        - d: número de palabras que tiene el documento 1 -> 6\n","        $$tf(t,d) = 1 + log(\\frac{3}{6}) =  0,69$$\n","        \n","    * ***IDF***:\n","        - n<sub>t</sub>: número de documentos en los que aparece la palabra 'messi' -> 2\n","        - D: número total de documentos en el corpus -> 2\n","        $$idf(t,D) = log(1 + \\frac{2}{2}) = 0,3$$\n","        \n","    * ***TF-IDF***:\n","    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,69 * 0,3 = 0,21$$\n","    \n","    \n","* ***Ejemplo 2***: Calculamos el ***tf-idf*** de la palabra \"**futbol**\" para el documento 2:\n","    \n","    * ***TF***:\n","        - t: número de veces que aparece la palabra \"futbol\" en el documento 2 -> 3\n","        - d: número de palabras que tiene el documento 1 -> 5\n","        $$tf(t,d) = 1 + log(\\frac{3}{5}) =  0,78$$\n","        \n","    * ***IDF***:\n","        - n<sub>t</sub>: número de documentos en los que aparece la palabra 'futbol' -> 1\n","        - D: número total de documentos en el corpus -> 2\n","        $$idf(t,D) = log(1 + \\frac{2}{1}) = 0,48$$\n","        \n","    * ***TF-IDF***:\n","    $$tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D) = 0,78 * 0,48 = 0,37$$\n","        \n","\n","## Implementación:\n","\n","* Las implementaciones del ***TF-IDF*** de **Scikit** y **Gensim** estan pensadas para corpus con un número relevante de documentos y de palabras, por tanto la implementación del ***TF-IDF*** acepta una serie de parámetros para no tener en cuenta Stop Words, palabras irrelevantes, etc. por lo que si se realiza una implementación del ***TF-IDF*** según la bibliografia no van a conincidir los resultados de esa implementación con los resultados de las librerías de **Scikit** y **Gensim** a no ser que se modifiquen los parámetros de las funciones del ***TF-IDF***.\n","\n","\n","Scikit\n","\n","* Las estructuras de datos de salida de ***Scikit*** son iguales de en el caso de la construcción del vector de frecuencias salvo que en el contenido de la matriz será de números decimales en vez de números enteros.\n","\n","\n","* Para más información ver el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"],"metadata":{"id":"bmRkFNtOVvlq"}},{"cell_type":"code","source":["corpus = [\"carro carro carro ur ur feliz\",\n","          \"carro ur fabian fabian fabian\"]"],"metadata":{"id":"if8ZrL_8A_J0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(min_df=1)\n","\n","# Pasamos los tweets normalizados a Bolsa de palabras\n","X = tfidf.fit_transform(corpus)\n","\n","# Resultados\n","print(tfidf.get_feature_names_out ())\n","print(X.toarray())"],"metadata":{"id":"TE0usIsoA_3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfidf = TfidfVectorizer(min_df=1)\n","\n","# Pasamos los tweets normalizados a Bolsa de palabras\n","X2 = tfidf.fit_transform(documento)\n","\n","# Resultados\n","print(tfidf.get_feature_names_out ())\n","print(X2.toarray())"],"metadata":{"id":"7Dwp4rSQBHV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### -Gensim\n","\n","* La estructura de datos de salida de ***Gensim*** es la misma que en el caso de la construcción de la Bolsa de Palabras de frecuencias salvo que los valores de las palabras en los documentos tendrán números decimales en vez de números enteros.\n","\n","\n","* Para más información ver el siguiente enlace: https://radimrehurek.com/gensim/models/tfidfmodel.html"],"metadata":{"id":"khvE4x8tVyUn"}},{"cell_type":"code","source":["import gensim\n","\n","tokenize = [nltk.word_tokenize(text) for text in documento]\n","dictionary = gensim.corpora.Dictionary(tokenize)\n","tfidf = gensim.models.TfidfModel(dictionary=dictionary, normalize=True)\n","vectors = [tfidf[dictionary.doc2bow(doc)] for doc in tokenize]\n","\n","# Resultados\n","print('Diccionario de palabras -> palabra: id\\n')\n","print(dictionary.token2id)\n","print('\\nApariciones de las palabras en los documentos (id, tfidf):')\n","vectors"],"metadata":{"id":"Xkmku2vwV041"},"execution_count":null,"outputs":[]}]}